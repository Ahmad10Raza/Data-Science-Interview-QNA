{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What was the purpose of Machine Learning?\n",
    "**The most straightforward response is to make our lives simpler. Many\n",
    "systems employed hardcoded rules of \"if\" and \"else\" choices to analyze\n",
    "data or change user input in the early days of \"intelligent\" applications.\n",
    "Consider a spam filter responsible for moving relevant incoming email\n",
    "messages to a spam folder. However, we provide enough data to learn and\n",
    "find patterns using machine learning algorithms.**\n",
    "\n",
    "**Unlike traditional challenges, we don't need to define new rules for each\n",
    "machine learning problem; instead, we need to utilize the same approach\n",
    "but with a different dataset.**\n",
    "\n",
    "For example, if we have a history dataset of real sales statistics, we may\n",
    "train machine learning models to forecast future sales.\n",
    "\n",
    "Principal Component Analysis, or PCA, is a dimensionality-reduction\n",
    "approach for reducing the dimensionality of big data sets by converting a\n",
    "large collection of variables into a smaller one that retains the majority of\n",
    "the information in the large set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Supervised Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning is a type of machine learning where the model is trained on a labeled dataset. A labeled dataset is one where each input data point has a corresponding target output. \n",
    "\n",
    "In Supervised Learning, the model learns to predict the output from the input data during the training process. The goal is to approximate the mapping function so well that when new input data is given, the model can predict the corresponding output.\n",
    "\n",
    "There are two main types of Supervised Learning tasks: \n",
    "\n",
    "1. **Regression**: The output is a continuous value. For example, predicting the price of a house based on features like number of rooms, location, etc.\n",
    "\n",
    "2. **Classification**: The output is a categorical value. For example, predicting whether an email is spam or not based on its content.\n",
    "\n",
    "The \"supervision\" in Supervised Learning comes from the fact that the model is provided with the correct answers (or labels) during training. The model makes predictions based on the input data, and the difference between the model's predictions and the actual output is used to adjust the model's parameters. This process is repeated until the model's predictions are as accurate as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several common algorithms used in Supervised Learning, each with its own strengths and use cases. Here are a few:\n",
    "\n",
    "1. **Linear Regression**: Used for regression problems, this algorithm models the relationship between a dependent variable and one or more independent variables using a best fit straight line.\n",
    "\n",
    "2. **Logistic Regression**: Despite its name, this is used for binary classification problems. It models the probability that a given input point belongs to a particular category.\n",
    "\n",
    "3. **Decision Trees**: Used for both regression and classification problems, this algorithm models decisions made based on certain conditions.\n",
    "\n",
    "4. **Random Forest**: An ensemble method that uses a multitude of decision trees. It outputs the class that is the mode of the classes output by individual trees for classification, or mean prediction for regression.\n",
    "\n",
    "5. **Support Vector Machines (SVM)**: Used for binary classification problems, this algorithm finds a hyperplane in a high-dimensional space that distinctly classifies the data points.\n",
    "\n",
    "6. **Naive Bayes**: Based on the Bayes Theorem, this algorithm is particularly suited when the dimensionality of the inputs is high.\n",
    "\n",
    "7. **K-Nearest Neighbors (KNN)**: Used for both regression and classification problems, this algorithm assumes similar things exist in close proximity.\n",
    "\n",
    "8. **Neural Networks**: Used for both regression and classification problems, these algorithms are particularly good at handling data that has complex, non-linear relationships.\n",
    "\n",
    "Remember, the choice of algorithm often depends on the size, quality, and nature of the data set, the insights you want to get from the data, and how those insights will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explain Unsupervised Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised Learning is a type of machine learning where the model is trained on an unlabeled dataset. An unlabeled dataset is one where the input data does not have a corresponding target output.\n",
    "\n",
    "The goal of Unsupervised Learning is to find patterns and structures in the input data. Since there are no target outputs, the model is not guided by a \"correct answer\". Instead, it must learn from the features of the input data.\n",
    "\n",
    "There are two main types of Unsupervised Learning tasks:\n",
    "\n",
    "1. **Clustering**: The goal is to group similar instances together into clusters. For example, a clustering algorithm can be used to segment customers into different groups based on their purchasing behavior.\n",
    "\n",
    "2. **Dimensionality Reduction**: The goal is to simplify the input data without losing too much information. One way to do this is to merge several correlated features into one. For example, a car's mileage may be very correlated with its age, so the dimensionality reduction algorithm will merge them into one feature that represents the car's wear and tear.\n",
    "\n",
    "Unsupervised Learning can be more challenging than Supervised Learning as it's harder to evaluate the results and ensure they're accurate. However, it can also be very powerful in situations where labeled data is not available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several common algorithms used in Unsupervised Learning, each with its own strengths and use cases. Here are a few:\n",
    "\n",
    "1. **K-Means Clustering**: This algorithm partitions the input data into K distinct clusters based on distance to the centroid of a cluster. The number of clusters, K, is user defined.\n",
    "\n",
    "2. **Hierarchical Clustering**: This algorithm builds a multilevel hierarchy of clusters by creating a cluster tree.\n",
    "\n",
    "3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: This algorithm groups together points that are packed closely together (points with many nearby neighbors), marking low-density regions that are far from any cluster as outliers.\n",
    "\n",
    "4. **Principal Component Analysis (PCA)**: This is a dimensionality reduction algorithm that is used to reduce the dimensionality of large datasets, while preserving as much information as possible.\n",
    "\n",
    "5. **Autoencoders**: These are a type of artificial neural network used for learning efficient codings of input data. They are typically used for dimensionality reduction and feature extraction.\n",
    "\n",
    "6. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: This is a machine learning algorithm for visualization and dimensionality reduction. It is particularly well suited for the visualization of high-dimensional datasets.\n",
    "\n",
    "Remember, the choice of algorithm often depends on the size, quality, and nature of the data set, the insights you want to get from the data, and how those insights will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. What should you do if you're Overfitting or Underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting and underfitting are common problems in machine learning. Here are some strategies to handle them:\n",
    "\n",
    "**Overfitting** occurs when your model learns the training data too well, including its noise and outliers, and performs poorly on unseen data. To combat overfitting:\n",
    "\n",
    "1. **Get more data**: More data can help the model generalize better.\n",
    "2. **Reduce complexity**: Simplify your model (fewer layers or neurons for neural networks, fewer features, simpler algorithm) to make it less prone to learning the noise in the data.\n",
    "3. **Regularization**: Techniques like L1 and L2 regularization add a penalty on the magnitude of model parameters, which can prevent overfitting.\n",
    "4. **Cross-validation**: Use cross-validation to get a more accurate estimate of the model's performance on unseen data.\n",
    "5. **Early stopping**: In gradient descent, stop training as soon as the validation error reaches a minimum.\n",
    "6. **Dropout**: In neural networks, randomly ignore some neurons during training to prevent overfitting.\n",
    "\n",
    "**Underfitting** occurs when your model is too simple to learn the underlying structure of the data. To combat underfitting:\n",
    "\n",
    "1. **Increase model complexity**: Use a more complex model or add more features.\n",
    "2. **Increase training time**: For neural networks, longer training might be necessary.\n",
    "3. **Feature engineering**: Create new features based on existing ones.\n",
    "4. **Remove regularization**: If you've applied regularization, it might be preventing the model from learning the data well.\n",
    "\n",
    "Remember, the key is to find a balance between bias (underfitting) and variance (overfitting) to achieve good predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Define Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Neural Network is a type of machine learning model inspired by the human brain. It's composed of interconnected layers of nodes, or \"neurons\", which take in input data and pass their signal to the next layer. The final layer produces the output.\n",
    "\n",
    "Each neuron receives input from multiple neurons from the previous layer. The inputs are multiplied by the neuron's weights, summed together, and then passed through an activation function to produce the neuron's output.\n",
    "\n",
    "Neural Networks can learn complex patterns and relationships in data, making them useful for a wide range of tasks, including image and speech recognition, natural language processing, and more.\n",
    "\n",
    "There are many types of Neural Networks, including:\n",
    "\n",
    "1. **Feedforward Neural Networks (FNNs)**: The information moves in only one direction, forward, from the input layer, through the hidden layers, to the output layer.\n",
    "\n",
    "2. **Convolutional Neural Networks (CNNs)**: Primarily used for image processing, pattern recognition, and machine vision tasks.\n",
    "\n",
    "3. **Recurrent Neural Networks (RNNs)**: They have connections that have loops, adding feedback and memory to the networks over time. This makes them ideal for time series analysis, natural language processing, etc.\n",
    "\n",
    "4. **Long Short-Term Memory Networks (LSTMs)**: A special kind of RNN that can learn and remember over long sequences, useful in sequence prediction problems.\n",
    "\n",
    "5. **Generative Adversarial Networks (GANs)**: Composed of two neural networks, a Generator and a Discriminator, that are trained together. The generator learns to generate fake data to fool the discriminator, while the discriminator learns to tell the real data from the fake.\n",
    "\n",
    "Training a Neural Network involves adjusting the weights and biases of the neurons based on the error of the network's output, typically using a process called backpropagation and an optimization technique such as gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. What is the meaning of Loss Function and Cost Function? What is the main distinction between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of machine learning, a **Loss Function**, also known as an error function, is a method of evaluating how well a specific algorithm models the given data. If predictions deviate too much from actual results, loss function would cough up a very large number. It is used to update the model parameters during training.\n",
    "\n",
    "A **Cost Function**, on the other hand, is the average of the loss functions of the entire dataset. It is a type of loss function, but it takes into account not just the prediction errors for a single data point, but for all data points. The goal of a machine learning model is to minimize the cost function.\n",
    "\n",
    "The main distinction between them is their usage. A loss function is used to calculate the error for a single training example while cost function is the average of loss functions for all the training examples. In other words, loss function is a part of the cost function which is a type of an average sum of loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE = √(predicted value - actual value)2\n",
    "\n",
    "Hinge loss: It is used to train the machine learning classifier, which is\n",
    "\n",
    "L(y) = max(0,1- yy)\n",
    "\n",
    "Where y = -1 or 1 denotes two classes and y denotes the classifier's output\n",
    "form. In the equation y = mx + b, the most common cost function depicts\n",
    "the entire cost as the sum of the fixed and variable costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the mathematical formulas for some common loss functions and the cost function:\n",
    "\n",
    "1. **Mean Squared Error (MSE)**: This is often used in regression problems. If `y` is the true value and `y_hat` is the predicted value, the MSE is calculated as:\n",
    "\n",
    "   ```math\n",
    "   MSE = 1/n * Σ(y - y_hat)^2\n",
    "   ```\n",
    "\n",
    "   where `n` is the number of data points, and `Σ` denotes the sum over all data points.\n",
    "\n",
    "2. **Cross-Entropy Loss**: This is often used in classification problems. For binary classification, the formula is:\n",
    "\n",
    "   ```math\n",
    "   Cross-Entropy Loss = -1/n * Σ[y * log(y_hat) + (1 - y) * log(1 - y_hat)]\n",
    "   ```\n",
    "\n",
    "   where `y` is the true label (0 or 1), `y_hat` is the predicted probability of the label being 1, and `n` is the number of data points.\n",
    "\n",
    "The **Cost Function** is simply the average of these loss functions over all data points. So, for example, if `L_i` is the loss for the `i-th` data point, the cost function `J` is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 1/n * ΣL_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "where `n` is the total number of data points, and `Σ` denotes the sum over all data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Define Ensemble Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Learning is a machine learning paradigm where multiple models (often called \"base learners\" or \"individual learners\") are trained to solve the same problem and combined to get better results. The main hypothesis behind ensemble methods is that when weak models are correctly combined we can obtain more accurate and/or robust models.\n",
    "\n",
    "The goal of Ensemble Learning is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability, robustness, and performance. \n",
    "\n",
    "There are several ways to create an ensemble model:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating)**: Bagging is a method that involves manipulating the training set by resampling. We create subsets of the original dataset (with replacement), train a model on each, and average the predictions. Random Forest is a popular example of a bagging-based ensemble method.\n",
    "\n",
    "2. **Boosting**: Boosting involves training models sequentially, where each new model is made to correct the errors made by the previous ones. Models are weighted based on their performance (models that predict correctly have higher weight). AdaBoost and Gradient Boosting are examples of boosting-based ensemble methods.\n",
    "\n",
    "3. **Stacking (Stacked Generalization)**: Stacking involves training models on the original dataset, then combining (or stacking) their predictions, and training a new model on these predictions to make a final prediction.\n",
    "\n",
    "4. **Voting**: Voting can be hard or soft. In hard voting, every individual classifier votes for a class, and the majority wins. In soft voting, every individual classifier provides a probability for each output class. The probabilities are averaged and the class with the highest probability is predicted.\n",
    "\n",
    "Ensemble methods can be computationally expensive, but they often achieve better performance than single models. They are widely used in a variety of fields and have proven to be very effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. How do you know the Machine Learning Algorithm you should use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is entirely dependent on the data we have. SVM is used when the data is\n",
    "discrete, and we utilize linear regression if the dataset is continuous. As a\n",
    "result, there is no one-size-fits-all method for determining which machine\n",
    "learning algorithm to utilize; it all relies on exploratory data analysis\n",
    "(EDA). EDA is similar to \"interviewing\" a dataset. We do the following as\n",
    "part of our interview:\n",
    "\n",
    "* Sort our variables into categories like continuous, categorical,\n",
    "and so on.\n",
    "* Use descriptive statistics to summarize our variables.\n",
    "* Use charts to visualize our variables.\n",
    "* Choose one best-fit method for a dataset based on the given\n",
    "observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right machine learning algorithm for a particular problem can be a challenging task. Here are some factors to consider:\n",
    "\n",
    "1. **Problem Type**: Is it a classification problem, a regression problem, clustering, or dimensionality reduction? Different algorithms are better suited for different types of problems.\n",
    "\n",
    "2. **Size of the Dataset**: Some algorithms can handle large datasets better than others. For example, linear models or tree-based models can be good for large datasets, while K-Nearest Neighbors or Support Vector Machines might not perform well on them.\n",
    "\n",
    "3. **Quality of the Dataset**: If your dataset has a lot of missing values or outliers, some algorithms might be more suitable than others. For example, tree-based models are not sensitive to outliers, while K-Nearest Neighbors and SVMs are.\n",
    "\n",
    "4. **Dimensionality of the Dataset**: If your dataset has a lot of features, you might need to use an algorithm that can handle high dimensionality (like Random Forest or Gradient Boosting), or a dimensionality reduction technique.\n",
    "\n",
    "5. **Interpretability**: If you need to explain the predictions to others, you might prefer simpler, more interpretable models like Linear Regression or Decision Trees.\n",
    "\n",
    "6. **Performance**: If the highest possible accuracy is the main goal, more complex models like Neural Networks or Ensemble Methods might be the best choice.\n",
    "\n",
    "Remember, there's no one-size-fits-all algorithm for every problem. It's often a good idea to try several algorithms and see which one works best for your specific problem. Also, using techniques like cross-validation can help you estimate how well your model will perform on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. How should Outlier Values be Handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers are data points that are significantly different from other observations. They might be caused by variability in the data or experimental errors. Here are some strategies to handle outliers:\n",
    "\n",
    "1. **Drop the outliers**: If you're certain that the outlier is due to incorrectly entered or measured data, you might want to drop these observations from your dataset.\n",
    "\n",
    "2. **Cap the outliers**: If you can determine a reasonable maximum and/or minimum value for your data, values outside this range can be set to these maximum and minimum values.\n",
    "\n",
    "3. **Transformation**: Applying a logarithmic or square root transformation can reduce the impact of outliers.\n",
    "\n",
    "4. **Binning**: Convert the data into bins to handle outliers. For example, you can convert numerical data into categorical data.\n",
    "\n",
    "5. **Imputation**: Replace the outlier with statistical measures such as the mean, median or mode.\n",
    "\n",
    "6. **Use robust models**: Some models are less sensitive to outliers than others. For example, tree-based models are not affected by outliers, while linear models and neural networks are.\n",
    "\n",
    "7. **Treat separately**: If outliers represent a significant portion of your data, it might be worth treating them as a separate group and build specific models for them.\n",
    "\n",
    "Remember, before handling outliers, it's important to investigate why they occurred and whether they might be important to your analysis. Sometimes, outliers can provide valuable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Define Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is a popular machine learning algorithm that belongs to the ensemble learning method. It involves constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "Here are some key points about Random Forest:\n",
    "\n",
    "1. **Multiple Decision Trees**: Random Forest builds multiple decision trees and merges them together. The main idea behind this is that a single decision tree might be prone to a noise, but aggregate of many decision trees reduce the effect of noise giving more accurate results.\n",
    "\n",
    "2. **Randomness**: Random Forest introduces randomness in two ways. First, it uses a method called bagging (bootstrap aggregating) to create different subsets of the original dataset, and a separate decision tree is trained on each subset. Second, instead of searching for the best feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model.\n",
    "\n",
    "3. **Reduces Overfitting**: By averaging out the predictions of a bunch of decision trees, mitigated by their individual errors, Random Forests tend to be very robust to overfitting.\n",
    "\n",
    "4. **Handle Large Datasets**: They can handle large datasets with high dimensionality effectively.\n",
    "\n",
    "5. **Feature Importance**: Random Forests are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection.\n",
    "\n",
    "Random Forests are widely used because of their simplicity and the fact that they can be used for both classification and regression tasks. They are also the foundation for more advanced ensemble methods such as Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the mechanism behind it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mechanism behind Random Forest involves multiple steps:\n",
    "\n",
    "1. **Bootstrapping the Data**: Random Forest starts by selecting random samples from the dataset with replacement, a process known as bootstrapping. This means some samples will be used multiple times in a single subset, and some might not be used at all.\n",
    "\n",
    "2. **Building Decision Trees**: For each bootstrap sample, a decision tree is built. When building the trees, at each node, a random subset of features are selected to determine the best split. This introduces randomness into the model creation process and ensures that the trees are uncorrelated.\n",
    "\n",
    "3. **Making Predictions**: To make a prediction, each tree in the forest makes its own individual prediction and the class with the most votes becomes the model’s prediction. For regression tasks, the final prediction could be the average of the predictions of all trees.\n",
    "\n",
    "This process of creating multiple models and making predictions based on a majority vote or averaging is what gives Random Forest its name. The \"Random\" part comes from the randomness introduced in creating the trees, and the \"Forest\" part comes from the large number of trees created.\n",
    "\n",
    "The mechanism behind Random Forest helps to ensure that the model is robust and less prone to overfitting. The randomness ensures that each individual tree is de-correlated from the others, while the process of averaging or majority voting reduces variance and therefore overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. What are SVM's different Kernels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM) is a powerful and versatile machine learning algorithm, capable of performing linear or nonlinear classification, regression, and even outlier detection. SVMs are particularly well suited for classification of complex small- or medium-sized datasets.\n",
    "\n",
    "SVM uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs. The kernel function is what is applied on each data instance to map the original non-linear observations into a higher-dimensional space in which they become separable.\n",
    "\n",
    "Here are the different types of kernels in SVM:\n",
    "\n",
    "1. **Linear Kernel**: A linear kernel can be used as normal dot product any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values.\n",
    "\n",
    "   ```math\n",
    "   K(x, xi) = sum(x * xi)\n",
    "   ```\n",
    "\n",
    "2. **Polynomial Kernel**: A polynomial kernel is a more generalized form of the linear kernel. The polynomial kernel can distinguish curved or nonlinear input space.\n",
    "\n",
    "   ```math\n",
    "   K(x,xi) = 1 + sum(x * xi)^d\n",
    "   ```\n",
    "\n",
    "   Where `d` is the degree of the polynomial. `d=1` is similar to the linear transformation. The degree needs to be manually specified in the learning algorithm.\n",
    "\n",
    "3. **Radial Basis Function Kernel (RBF)**: The Radial basis function kernel is a popular kernel function commonly used in support vector machine classification. RBF can map an input space in infinite dimensional space.\n",
    "\n",
    "   ```math\n",
    "   K(x,xi) = exp(-gamma * sum((x – xi^2))\n",
    "   ```\n",
    "\n",
    "   Here `gamma` is a parameter, which ranges from 0 to 1. A higher value of gamma will perfectly fit the training dataset, which causes over-fitting. Gamma=0.1 is considered to be a good default value. The value of gamma needs to be manually specified in the learning algorithm.\n",
    "\n",
    "4. **Sigmoid Kernel**: Sigmoid kernel is similar to the sigmoid function in logistic regression.\n",
    "\n",
    "   ```math\n",
    "   K(x,xi) = tanh(alpha*x*xi + c)\n",
    "   ```\n",
    "\n",
    "   Here `alpha` and `c` are constants. `alpha` > 0 and `c` < 0.\n",
    "\n",
    "Choosing the right kernel function is a crucial part of tuning an SVM model. The choice of kernel depends on the nature of the problem, the dimensionality of the data, and the number of training examples available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
