{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General ML Questions and Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 Define Sampling. Why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling is a statistical method that involves selecting a subset of individuals from a larger population to estimate characteristics of the whole population. It is a fundamental aspect of data collection in many fields, including machine learning and data science.\n",
    "\n",
    "We need sampling for several reasons:\n",
    "\n",
    "1. **Feasibility**: It is often impractical or impossible to collect data from every individual in a population. Sampling provides a method to gather information about the population in a manageable, cost-effective way.\n",
    "\n",
    "2. **Efficiency**: Sampling allows us to get a good estimate of the population characteristics with less data. This can significantly reduce the time and resources required for data collection and processing.\n",
    "\n",
    "3. **Accuracy**: If done correctly, sampling can provide accurate estimates of the population. In some cases, sampling can even be more accurate than attempting to collect data from the entire population, as it can reduce the impact of errors or anomalies that might occur in a full census.\n",
    "\n",
    "4. **Speed**: In machine learning, working with smaller datasets can significantly speed up the process of model training and testing, allowing for faster iteration and improvement of models.\n",
    "\n",
    "Remember, the key to effective sampling is to ensure that the sample is representative of the population. This often involves using random sampling methods to reduce bias and ensure that every individual in the population has an equal chance of being included in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17 What is stratified sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified sampling is a method of sampling that involves dividing the population into homogeneous subgroups known as `strata`, and then taking a random sample from each stratum.\n",
    "\n",
    "The strata are usually formed based on one or more characteristics of the population members, such as age, gender, socioeconomic status, etc. The goal is to ensure that each stratum is representative of that characteristic in the population.\n",
    "\n",
    "Stratified sampling is used when the population is not homogeneous or uniformly distributed and has some form of variance. By ensuring that each subgroup is adequately represented in the sample, stratified sampling can provide more accurate and reliable estimates than simple random sampling, especially for population characteristics that vary between subgroups.\n",
    "\n",
    "For example, if you were conducting a survey on smoking habits and you know that smoking rates vary significantly by age group, you might use stratified sampling to ensure that your sample includes a representative number of individuals from each age group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18 Define Confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confidence interval is a range of values, derived from a statistical procedure, that is likely to contain an unknown population parameter. It provides an estimated range of values which is likely to include the unknown parameter, such as a population mean or proportion.\n",
    "\n",
    "The confidence interval is usually stated with a confidence level. The confidence level represents the frequency (i.e., the proportion) of possible confidence intervals that contain the true value of the unknown population parameter. For example, if you use a confidence level of 95%, then 95% of the confidence intervals you calculate from this process will contain the true value of the parameter.\n",
    "\n",
    "In other words, a 95% confidence interval represents a range of values that you can be 95% certain contains the true mean of the population. It's important to note that the confidence level is not a measure of the probability that the true value lies within the interval - instead, it describes how confident you are that the procedure used to construct the interval will capture the true value if you repeated the procedure multiple times.\n",
    "\n",
    "For example, if a confidence interval for the mean weight of a species of fish is (4.5 kg, 5.5 kg) at a 95% confidence level, it means that we are 95% confident that the true mean weight of all fishes of this species lies within this interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19 What do you mean by i.i.d. assumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The i.i.d. assumption stands for Independent and Identically Distributed. This is a statistical term that is often used in the context of machine learning and data analysis. Let's break it down:\n",
    "\n",
    "1. **Independent**: The independence assumption is that each sample in your dataset does not depend on the others. For example, the outcome of a coin toss does not depend on the results of the previous tosses.\n",
    "\n",
    "2. **Identically Distributed**: This means that each sample comes from the same probability distribution. In other words, the underlying probability distribution from which the samples are drawn is assumed to be the same for each sample.\n",
    "\n",
    "The i.i.d. assumption is crucial for many statistical and machine learning models because it greatly simplifies the mathematics and theory involved. However, it's important to note that in real-world data, this assumption is often violated to some degree. For example, in time series data, measurements taken close together in time may be correlated, violating the independence assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Why do we call it GLM (Generalized Linear Model) when it is clearly nonlinear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term \"Generalized Linear Model\" (GLM) might seem confusing because GLMs can indeed model relationships that are nonlinear in the predictors. However, the \"linear\" in GLM refers to the linearity in the parameters, not in the predictors.\n",
    "\n",
    "In a GLM, we model the relationship between the predictors and a transformation of the expected value of the response variable, not the response variable itself. This transformation is known as the link function. The link function is what allows GLMs to model non-linear relationships.\n",
    "\n",
    "For example, in logistic regression (a type of GLM), we model the log-odds (the link function) as a linear function of the predictors. The relationship between the predictors and the response (probability of success) is clearly nonlinear, but the relationship between the predictors and the log-odds is linear.\n",
    "\n",
    "So, in summary, GLMs are called \"linear\" because they model a linear relationship between the predictors and a transformed version of the response. The transformation allows GLMs to model a wider range of relationships than ordinary linear regression, while still maintaining the benefits of a linear model, such as ease of interpretation and computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21 Define Conditional Probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional probability is a measure of the probability of an event occurring, given that another event has already occurred. If the event of interest is A and event B has already occurred, the conditional probability of A given B is usually written as P(A|B).\n",
    "\n",
    "The concept of conditional probability is one of the fundamental concepts in probability theory and statistics. It forms the basis for many important topics such as Bayes' theorem, Markov Chains, and many machine learning algorithms.\n",
    "\n",
    "The formula for conditional probability is:\n",
    "\n",
    "P(A|B) = P(A ∩ B) / P(B)\n",
    "\n",
    "where:\n",
    "- P(A|B) is the conditional probability of A given B,\n",
    "- P(A ∩ B) is the probability of both A and B occurring,\n",
    "- P(B) is the probability of B occurring.\n",
    "\n",
    "It's important to note that P(A|B) is not necessarily equal to P(B|A). That is, the probability of A occurring given that B has occurred may not be the same as the probability of B occurring given that A has occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22 Are you familiar with Bayes Theorem? Can you tell me why is it useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, I'm familiar with Bayes' Theorem. It's a fundamental concept in probability theory and statistics that describes how to update the probabilities of hypotheses when given evidence. It's named after Thomas Bayes, who provided the first mathematical formulation of the theorem.\n",
    "\n",
    "The theorem is stated mathematically as follows:\n",
    "\n",
    "P(A|B) = [P(B|A) * P(A)] / P(B)\n",
    "\n",
    "Where:\n",
    "- P(A|B) is the conditional probability of event A given event B.\n",
    "- P(B|A) is the conditional probability of event B given event A.\n",
    "- P(A) and P(B) are the probabilities of events A and B respectively.\n",
    "\n",
    "Bayes' Theorem is useful in a wide range of applications:\n",
    "\n",
    "1. **Machine Learning**: It forms the backbone of Bayesian inference, which is used in Bayesian Neural Networks, Naive Bayes classifiers, and other Bayesian models.\n",
    "\n",
    "2. **Data Science**: It's used in A/B testing, prediction modeling, and other statistical analyses.\n",
    "\n",
    "3. **Other Fields**: It's used in medical testing, search and rescue operations, code breaking, and more.\n",
    "\n",
    "In essence, Bayes' Theorem provides a way to update our beliefs in light of new evidence, making it a powerful tool for making predictions based on incomplete information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23 How can you get an unbiased estimate of the accuracy of the learned model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an unbiased estimate of the accuracy of a learned model, you can use techniques such as cross-validation, holdout method, or bootstrapping. Here's a brief explanation of each:\n",
    "\n",
    "1. **Cross-Validation**: This is a popular method where the dataset is randomly divided into 'k' subsets or folds. The model is trained on k-1 folds, and the remaining fold is used for testing. This process is repeated k times, each time with a different fold used as the testing set. The average performance across all k iterations gives an unbiased estimate of the model accuracy.\n",
    "\n",
    "2. **Holdout Method**: In this method, the dataset is divided into two sets: a training set and a testing set (often in a 70:30 or 80:20 ratio). The model is trained on the training set and tested on the testing set. This method is simple and fast, but the estimate of the model accuracy can vary a lot depending on how the data is split.\n",
    "\n",
    "3. **Bootstrapping**: This is a resampling technique where multiple subsets of the original dataset are created by sampling with replacement, and the model is trained and tested on each of these subsets. The average performance across all subsets gives an estimate of the model accuracy.\n",
    "\n",
    "Remember, it's important to ensure that the test set is not used in any way to make decisions about the model (like choosing hyperparameters) because that can lead to overfitting and an overly optimistic estimate of the model accuracy. For making such decisions, a separate validation set can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24 How would you handle the scenario where your dataset has missing or dirty (garbled) values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing or dirty data is a common task in data preprocessing. Here are some strategies:\n",
    "\n",
    "1. **Remove**: If only a few observations have missing or dirty values, you might consider removing those observations. However, this isn't a good strategy if a lot of observations have missing values as you could lose significant data.\n",
    "\n",
    "2. **Impute**: Replace missing values with substituted values. The simplest strategy is to use the mean, median, or mode of the feature. For a more sophisticated approach, you can use a method like k-nearest neighbors, where the missing value is replaced with a value from similar observations.\n",
    "\n",
    "3. **Predict**: Use a machine learning algorithm to predict the missing values. This could be a simple linear regression, or a more complex model like a Random Forest.\n",
    "\n",
    "4. **Ignore**: If the feature with missing values isn't crucial to your analysis, you might choose to ignore that feature.\n",
    "\n",
    "For dirty data, the strategy would depend on the nature of the 'dirt'. It could involve:\n",
    "\n",
    "1. **Manual cleaning**: If the dataset is small, manual cleaning might be feasible. This involves going through the dataset and fixing errors.\n",
    "\n",
    "2. **Automated cleaning**: This could involve writing scripts to clean the data. For example, if dates are in different formats, you could write a script to standardize them.\n",
    "\n",
    "3. **Outlier detection**: If the dirty data is due to outliers, outlier detection algorithms could help.\n",
    "\n",
    "4. **Data validation**: Implementing data validation rules can prevent dirty data. For example, if a certain field should always be a positive number, a data validation rule could enforce this.\n",
    "\n",
    "Remember, it's important to understand why the data is missing or dirty. Is it missing at random, or is there a pattern to the missing data? Understanding the underlying reason can help you choose the best strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25 What is EM algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Expectation-Maximization (EM) algorithm is a statistical technique used for finding maximum likelihood estimates of parameters in probabilistic models, where the model depends on unobserved latent variables. It's particularly useful when the data are missing or incomplete.\n",
    "\n",
    "The EM algorithm consists of two steps: the Expectation step (E-step) and the Maximization step (M-step), which are repeated iteratively until the algorithm converges.\n",
    "\n",
    "1. **E-step**: Given the current estimates for the parameters, the E-step computes the expected value of the log-likelihood function.\n",
    "\n",
    "2. **M-step**: The M-step then updates the parameter estimates by maximizing the expected log-likelihood found on the E-step. The new parameter estimates are used in the next E-step.\n",
    "\n",
    "The EM algorithm is guaranteed to increase the likelihood at each step, and it will converge to a local maximum.\n",
    "\n",
    "EM is used in various applications such as clustering, image processing, natural language processing, and bioinformatics. A well-known application of the EM algorithm is in Gaussian Mixture Models, which is a method used for clustering data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26 How are True Positive Rate and Recall related?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Positive Rate (TPR) and Recall are actually the same concept, just with different names. Both of them measure the proportion of actual positives (True Positives) that are correctly identified as such.\n",
    "\n",
    "Here's the formula for both:\n",
    "\n",
    "TPR / Recall = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "- True Positives (TP) are the correctly predicted positive values.\n",
    "- False Negatives (FN) are the negatives which were predicted as positive but are actually negative.\n",
    "\n",
    "In the context of binary classification:\n",
    "\n",
    "- Recall is often used in machine learning and is one of the metrics that make up the confusion matrix. It's also known as Sensitivity or Hit Rate.\n",
    "- True Positive Rate is often used in statistics and is the equivalent of Recall in the ROC (Receiver Operating Characteristic) curve.\n",
    "\n",
    "So, in summary, TPR and Recall are the same thing, just used in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27 Differentiate between ROC curve and PR curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC (Receiver Operating Characteristic) curve and PR (Precision-Recall) curve are both used for evaluating the performance of a binary classification model, but they are used in different scenarios and highlight different aspects of the model.\n",
    "\n",
    "1. **ROC Curve**: The ROC curve plots the True Positive Rate (TPR, or Recall) against the False Positive Rate (FPR) at various threshold settings. The area under the ROC curve (AUC-ROC) is a measure of how well a parameter can distinguish between two diagnostic groups (diseased/normal). The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n",
    "\n",
    "2. **PR Curve**: The PR curve plots Precision (Positive Predictive Value) against Recall (TPR). It's used when the classes are imbalanced. In imbalanced datasets, a high AUC-ROC value can be misleading as the model might simply be predicting the majority class well. In such cases, the PR curve can provide a more informative picture of model performance.\n",
    "\n",
    "In summary, while ROC curves should be used when there are roughly equal numbers of observations for each class, Precision-Recall curves should be used when there is a moderate to large class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28 What is a probabilistic graphical model? What is the difference between Markov Networks and Bayesian Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Probabilistic Graphical Model (PGM) is a type of model that expresses the conditional dependence structure between random variables using a graph. They are used in a wide range of applications, from computer vision to natural language processing. There are two main types of PGMs: Directed and Undirected.\n",
    "\n",
    "1. **Directed PGMs (Bayesian Networks)**: In a Bayesian Network, the graph is a Directed Acyclic Graph (DAG), and the edges represent conditional dependencies. If there is an edge from node A to node B, it means that B is conditionally dependent on A. The joint distribution of a Bayesian Network is factored into the product of conditional probabilities.\n",
    "\n",
    "2. **Undirected PGMs (Markov Networks)**: In a Markov Network, the graph is undirected, and the edges represent correlations between variables. If there is an edge between nodes A and B, it means that A and B are correlated given all other nodes. The joint distribution of a Markov Network is factored into the product of functions of multiple variables, known as clique potentials.\n",
    "\n",
    "The main difference between Bayesian Networks and Markov Networks is the directionality of the edges (Bayesian Networks have directed edges, Markov Networks have undirected edges), which leads to different interpretations and factorizations of the joint distribution. Bayesian Networks are more suited to represent causal relationships, while Markov Networks are more suited to represent soft correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29 Define non-negative matrix factorization. Give an example of its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-negative Matrix Factorization (NMF) is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect and leads to parts-based representations because they allow only additive, not subtractive, combinations.\n",
    "\n",
    "Mathematically, for a given non-negative matrix V, NMF finds an approximation:\n",
    "\n",
    "V ≈ W * H\n",
    "\n",
    "where V is an n x m matrix, W is an n x k matrix, and H is a k x m matrix. The factorization is done in such a way that W and H are non-negative, and the approximation error is minimized.\n",
    "\n",
    "One of the key applications of NMF is in text mining, where it's used for topic modeling. In this context, the matrix V could represent the frequency of words in documents, where each row corresponds to a document and each column corresponds to a word. The matrix factorization then results in two matrices: W, where each row represents a document and each column represents a topic, and H, where each row represents a topic and each column represents a word. This allows us to see which topics are present in each document and which words are associated with each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  30 How is k-Nearest Neighbors (k-NN) different from k-Means algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Nearest Neighbors (k-NN) and k-Means are both popular algorithms in machine learning, but they serve different purposes and work in different ways.\n",
    "\n",
    "1. **k-Nearest Neighbors (k-NN)**: k-NN is a type of instance-based learning algorithm used for classification and regression. Given a new, unknown observation, k-NN goes into the training set to find the k training examples that are closest to the new instance, and assigns to the new observation the most common (in classification) or average (in regression) output value among these k neighbors.\n",
    "\n",
    "2. **k-Means**: k-Means is a type of centroid-based clustering algorithm. The goal of k-Means is to partition the data into k groups (clusters) such that the total sum of the squared distances from each point to the mean point of its assigned cluster is minimized.\n",
    "\n",
    "The key differences between k-NN and k-Means are:\n",
    "\n",
    "- Purpose: k-NN is a supervised learning algorithm used for classification or regression, while k-Means is an unsupervised learning algorithm used for clustering.\n",
    "- Functioning: k-NN works by finding the most similar instances in the training set for a given test instance, while k-Means works by finding the best centroids to separate the data into clusters.\n",
    "- Output: k-NN outputs a class label or a continuous value, while k-Means outputs a set of cluster centers and assigns each instance to a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
