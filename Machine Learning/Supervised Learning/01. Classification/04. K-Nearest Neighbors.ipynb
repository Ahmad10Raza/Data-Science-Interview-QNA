{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **K-Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Walk me through the k-Nearest Neighbors algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-Nearest Neighbors (k-NN) algorithm is a type of instance-based learning algorithm used for both classification and regression. Here's a step-by-step walkthrough of how it works:\n",
    "\n",
    "1. **Choose the number of k and a distance metric**: The algorithm starts by initializing the number of neighbors (k) it will use for voting or averaging, and the type of distance metric (such as Euclidean or Manhattan distance).\n",
    "\n",
    "2. **Find the k-nearest neighbors**: For each instance that we need to make a prediction for, the algorithm identifies the k instances in the training dataset that are closest to it. The distance is calculated using the distance metric chosen in step 1.\n",
    "\n",
    "3. **Make a prediction**: \n",
    "   - For classification, the algorithm assigns the class label by majority vote, i.e., the most common class label among the k-nearest neighbors is assigned to the instance.\n",
    "   - For regression, the algorithm assigns the output to be the average of the values of the k nearest neighbors.\n",
    "\n",
    "4. **Return the prediction**: The algorithm returns the prediction for the instance, and this process is repeated for each instance in the test set.\n",
    "\n",
    "The k-NN algorithm is simple and easy to understand, but there are a few important things to consider:\n",
    "\n",
    "- Choosing the right value for k is crucial. A small value of k means that noise will have a higher influence on the result, and a large value makes it computationally expensive.\n",
    "- Distance measure is also important. Common choices are Euclidean distance for continuous variables and Hamming distance for categorical variables.\n",
    "- The algorithm is sensitive to irrelevant or redundant features because all features contribute to the calculation of the distance.\n",
    "- k-NN can be slow for large datasets because it calculates the distance between a test point and each training point.\n",
    "- Feature scaling is important in k-NN as features with higher scales can dominate the distance metric."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
