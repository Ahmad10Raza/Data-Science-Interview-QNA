{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Define Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a statistical and machine learning technique for binary classification problems. It's a predictive analysis algorithm based on the concept of probability.\n",
    "\n",
    "Here's a step-by-step explanation of how it works:\n",
    "\n",
    "1. **Linear Transformation**: Logistic Regression begins by transforming the input data linearly, similar to Linear Regression. It calculates a weighted sum of the input features plus a bias term.\n",
    "\n",
    "2. **Logistic Function**: The result of the linear transformation is then passed through a logistic function (also known as the sigmoid function). The logistic function outputs a value between 0 and 1.\n",
    "\n",
    "3. **Probability Interpretation**: The output of the logistic function is interpreted as the probability of the input data belonging to the positive class. If the probability is greater than a certain threshold (commonly 0.5), the data is classified as the positive class, otherwise, it's classified as the negative class.\n",
    "\n",
    "4. **Parameter Learning**: The weights in the linear transformation are learned from the training data using a technique called Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "Logistic Regression is simple, fast, and efficient for small datasets with binary classification. It works best when the predictors are independent of each other, and it's less prone to overfitting. However, it can't solve complex problems that have multiple or non-linear decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 How do you train a Logistic Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Logistic Regression model involves estimating the parameters (weights and bias) that minimize the error between the model's predictions and the actual data. This is typically done using a method called Maximum Likelihood Estimation (MLE). Here's a step-by-step process:\n",
    "\n",
    "1. **Initialize Parameters**: Start with initial estimates for the weights and bias. These can be set to zero or a small random number.\n",
    "\n",
    "2. **Calculate Predictions**: For each instance in the training set, calculate the prediction using the current weights and bias and the logistic function.\n",
    "\n",
    "3. **Compute the Loss**: The loss function for Logistic Regression is typically the log loss, which measures the difference between the actual class and the predicted probability. Compute the loss for each instance in the training set and then compute the average loss across all instances.\n",
    "\n",
    "4. **Calculate Gradients**: Compute the gradient of the loss function with respect to each weight and bias. The gradient is a measure of how much the loss will change if you change the weights and bias by a small amount.\n",
    "\n",
    "5. **Update Parameters**: Update the weights and bias by a small step in the direction that decreases the loss. The size of the step is determined by the learning rate, a hyperparameter that you must set.\n",
    "\n",
    "6. **Repeat Steps 2-5**: Repeat the process of making predictions, computing the loss, calculating gradients, and updating parameters until the model's predictions are good enough (i.e., the loss is below a certain threshold) or the maximum number of iterations has been reached.\n",
    "\n",
    "7. **Model Evaluation**: Evaluate the performance of the model on a separate validation set. Adjust the threshold value to get the best precision and recall trade-off.\n",
    "\n",
    "Remember, Logistic Regression is a linear model, which means it will not perform well if there are non-linear relationships between the features and the target variable. In such cases, you might need to create polynomial features or consider using a non-linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 How do you estimate the coefficients in Logistic Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Logistic Regression, the coefficients (or parameters) are estimated using a method called Maximum Likelihood Estimation (MLE). Here's a step-by-step process:\n",
    "\n",
    "1. **Initialize Coefficients**: Start with initial estimates for the coefficients. These can be set to zero or a small random number.\n",
    "\n",
    "2. **Calculate Log-Likelihood**: For each instance in the training set, calculate the log-likelihood using the current coefficients. The log-likelihood is a measure of how probable the observed data is given the current coefficients.\n",
    "\n",
    "3. **Compute the Gradient of the Log-Likelihood**: Compute the gradient of the log-likelihood with respect to each coefficient. The gradient is a measure of how much the log-likelihood will change if you change the coefficients by a small amount.\n",
    "\n",
    "4. **Update Coefficients**: Update the coefficients by a small step in the direction that increases the log-likelihood. The size of the step is determined by the learning rate, a hyperparameter that you must set.\n",
    "\n",
    "5. **Repeat Steps 2-4**: Repeat the process of calculating the log-likelihood, computing the gradient, and updating coefficients until the log-likelihood converges (i.e., it doesn't change much from one iteration to the next) or the maximum number of iterations has been reached.\n",
    "\n",
    "This process is typically done using an optimization algorithm like Gradient Descent or a more advanced method like Newton's Method or Quasi-Newton Methods (like BFGS or L-BFGS).\n",
    "\n",
    "The goal of MLE is to find the coefficients that make the observed data most probable, hence, making the model's predictions as accurate as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 How would you evaluate a Logistic Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating a Logistic Regression model involves assessing its performance on unseen data. Here are some common metrics used for evaluation:\n",
    "\n",
    "1. **Accuracy**: This is the proportion of total predictions that are correctly predicted. It's a good measure when the target classes are balanced.\n",
    "\n",
    "2. **Confusion Matrix**: This is a table that describes the performance of a classification model. It includes terms like True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "\n",
    "3. **Precision**: This is the proportion of positive predictions that are actually correct. It is TP/(TP+FP).\n",
    "\n",
    "4. **Recall (Sensitivity)**: This is the proportion of actual positive cases which are correctly predicted. It is TP/(TP+FN).\n",
    "\n",
    "5. **F1 Score**: This is the harmonic mean of Precision and Recall and provides a balance between the two.\n",
    "\n",
    "6. **ROC Curve**: This is a plot of the true positive rate (Recall) against the false positive rate (1-Specificity) for different threshold values.\n",
    "\n",
    "7. **AUC-ROC Score**: Area Under the ROC Curve (AUC-ROC) is a single value summary of the model performance. A value of 1 indicates a perfect model, and a value of 0.5 indicates a model that is no better than random guessing.\n",
    "\n",
    "8. **Log Loss**: This is the loss function used in Logistic Regression. It's a good measure to evaluate the probabilities predicted by the model.\n",
    "\n",
    "Remember, the choice of evaluation metric depends on the problem and the business context. For example, in a problem where false positives are more costly than false negatives, you might want to minimize FP, which means you should focus on Precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 What is a link function in Logistic Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Logistic Regression, the link function is used to connect the linear predictor function with the probability of the positive class. The link function used in Logistic Regression is the logistic function, also known as the sigmoid function.\n",
    "\n",
    "The logistic function transforms any real-valued number to the (0, 1) range, which can be used to represent a probability. The function is defined as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 1 / (1 + e^-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. The linear part of the model (i.e., the weighted sum of the features) produces a real-valued output.\n",
    "\n",
    "2. This real-valued output is then passed through the logistic function.\n",
    "\n",
    "3. The logistic function maps the real-valued number to a value between 0 and 1.\n",
    "\n",
    "4. This output can be interpreted as the probability of the positive class.\n",
    "\n",
    "The use of the logistic function is what makes Logistic Regression a probabilistic classifier, meaning it not only predicts the class label, but also provides a level of certainty (probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 What is the range of the Logistic Regression function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of the Logistic Regression function, also known as the sigmoid function, is between 0 and 1 (exclusive). \n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 1 / (1 + e^-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As x approaches infinity, e^-x approaches 0, and therefore f(x) approaches 1. Conversely, as x approaches negative infinity, e^-x approaches infinity, and therefore f(x) approaches 0. \n",
    "\n",
    "This property of the sigmoid function makes it particularly useful in Logistic Regression, as the output can be interpreted as a probability that the given input point belongs to a certain class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 When is Logistic Regression multinomial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is multinomial when the target variable has more than two categories. In standard Logistic Regression, the target variable has two possible outcomes (binary classification). However, in many real-world problems, the target variable can have more than two categories.\n",
    "\n",
    "Multinomial Logistic Regression is an extension of Logistic Regression that allows for more than two categories of the dependent variable. It works by modeling the probability of the categorical dependent variable in terms of one or more independent variables.\n",
    "\n",
    "In Multinomial Logistic Regression, one category is chosen as the reference category, and the model estimates the probabilities of the other categories relative to this reference category. The model uses a softmax function instead of a sigmoid function to estimate these probabilities. The softmax function can handle multiple classes and ensures that the estimated probabilities sum up to 1 across all categories for each observation.\n",
    "\n",
    "It's important to note that Multinomial Logistic Regression assumes that the categories of the target variable are unordered. If the categories have a natural order (like 'low', 'medium', 'high'), then Ordinal Logistic Regression would be a more appropriate technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Tell me about One vs All Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-vs-All (also known as One-vs-Rest) is a strategy for using binary classification algorithms, like Logistic Regression, for multi-class classification problems.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. For each class in the dataset, a separate binary Logistic Regression model is trained. The model is trained to predict whether an instance belongs to that class (positive class) or not (negative class).\n",
    "\n",
    "2. To make a prediction for a new instance, all the classifiers are run. Each classifier estimates a probability that the new instance belongs to its positive class.\n",
    "\n",
    "3. The classifier that outputs the highest probability is selected, and the corresponding class is returned as the prediction.\n",
    "\n",
    "For example, if you have a three-class problem with classes A, B, and C, you would train three binary classifiers: A vs [B, C], B vs [A, C], and C vs [A, B].\n",
    "\n",
    "One-vs-All is a simple and effective strategy for multi-class classification. However, it can be computationally expensive for large datasets with many classes, as it requires training one classifier for each class. Also, it assumes that each classification problem (i.e., each set of one class vs the rest) is equally difficult, which may not be the case in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[5.02044778e-01 4.97946962e-01 8.26045820e-06]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "[X, y] = datasets.load_iris(return_X_y=True)\n",
    "# Create one-vs-rest logistic regression model and train it\n",
    "clf = LogisticRegression(random_state=0, multi_class='ovr')\n",
    "clf.fit(X, y)\n",
    "# Create new observation\n",
    "new_observation = [[.2, .4, .6, .8]]\n",
    "# Let's predict its class\n",
    "print(clf.predict(new_observation))\n",
    "# The probability of the class predicted should be the highest among all the probabilities.\n",
    "print(clf.predict_proba(new_observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Which metric is generally used to evaluate the performance of a Logistic Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of metric to evaluate the performance of a Logistic Regression model depends on the specific problem and the business context. However, some commonly used metrics include:\n",
    "\n",
    "1. **Accuracy**: This is the proportion of total predictions that are correctly predicted. It's a good measure when the target classes are balanced.\n",
    "\n",
    "2. **Precision, Recall, and F1 Score**: These metrics are useful when the classes are imbalanced. Precision is the proportion of positive predictions that are actually correct. Recall (or Sensitivity) is the proportion of actual positive cases which are correctly predicted. The F1 Score is the harmonic mean of Precision and Recall, providing a balance between the two.\n",
    "\n",
    "3. **AUC-ROC**: The Area Under the Receiver Operating Characteristic Curve (AUC-ROC) is a performance measurement for classification problems at various thresholds settings. It tells how much the model is capable of distinguishing between classes.\n",
    "\n",
    "4. **Log Loss**: This is the loss function used in Logistic Regression. It's a good measure to evaluate the probabilities predicted by the model.\n",
    "\n",
    "Remember, no single metric can provide a complete picture of the model's performance. It's often useful to consider multiple metrics and understand the trade-offs between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 What would you do to speed up your algorithm without compromising a lot on the model’s accuracy, if your training dataset is huge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your training dataset is huge and you want to speed up your Logistic Regression algorithm without compromising a lot on the model’s accuracy, you can consider the following strategies:\n",
    "\n",
    "1. **Feature Selection**: Reduce the number of features in your dataset by keeping only the most relevant features. This can be done using techniques like correlation analysis, Recursive Feature Elimination (RFE), or using feature importance from a tree-based model.\n",
    "\n",
    "2. **Dimensionality Reduction**: Techniques like Principal Component Analysis (PCA) can reduce the dimensionality of your data by creating new features that maximize the variance in the data.\n",
    "\n",
    "3. **Sampling**: Use a representative subset of your data for training. Techniques like stratified sampling ensure that the sample is representative of the class distribution in the full dataset.\n",
    "\n",
    "4. **Regularization**: Use regularization techniques (like L1 or L2 regularization) to prevent overfitting and reduce the complexity of the model.\n",
    "\n",
    "5. **Optimization Algorithm**: Use a more efficient optimization algorithm. Stochastic Gradient Descent (SGD) or Mini-Batch Gradient Descent can be faster than Batch Gradient Descent for large datasets.\n",
    "\n",
    "6. **Parallelization**: If possible, leverage parallel processing capabilities of your hardware to train models on subsets of the data simultaneously.\n",
    "\n",
    "7. **Use a simpler model**: If model accuracy is not critically important, consider using a simpler model. Simpler models often train faster than more complex ones.\n",
    "\n",
    "Remember, there's always a trade-off between speed and accuracy. The key is to find a balance that's acceptable for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
