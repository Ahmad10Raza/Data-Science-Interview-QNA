{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASTERING THE BASICS OF `Statistics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What criteria do we use to determine the statistical importance of an instance?\n",
    "The statistical significance of insight is determined by hypothesis testing.\n",
    "The null and alternate hypotheses are provided, and the p-value is computed\n",
    "to explain further. We considered a null hypothesis true after computing the\n",
    "p-value, and the values were calculated. The alpha value, which indicates\n",
    "importance, is changed to fine-tune the outcome. The null hypothesis is\n",
    "rejected if the p-value is smaller than the alpha. As a consequence, the\n",
    "given result is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the statistical importance of an instance involves various criteria depending on the context and the specific statistical method being used. Here are some common criteria:\n",
    "\n",
    "1. **P-Value:**\n",
    "   - In hypothesis testing, the p-value measures the probability of observing a test statistic as extreme as the one computed from the sample data, assuming the null hypothesis is true. A low p-value (typically below a chosen significance level, e.g., 0.05) suggests that the instance is statistically significant.\n",
    "\n",
    "2. **Confidence Intervals:**\n",
    "   - Examining the confidence interval around a point estimate provides a range within which the true value is likely to fall. If the interval does not contain zero or a certain reference value, the instance may be considered statistically significant.\n",
    "\n",
    "3. **Effect Size:**\n",
    "   - Effect size measures the strength of a relationship or the magnitude of an observed effect. Large effect sizes are often considered more statistically important than smaller ones.\n",
    "\n",
    "4. **Coefficient Significance:**\n",
    "   - In regression analysis, the significance of coefficients is assessed. If the p-value associated with a coefficient is below a chosen threshold (e.g., 0.05), it indicates the variable's statistical importance in explaining the variation in the dependent variable.\n",
    "\n",
    "5. **Bayesian Credible Intervals:**\n",
    "   - In Bayesian statistics, credible intervals provide a range of values within which the true parameter value is likely to lie. Instances with credible intervals excluding certain values are considered statistically significant.\n",
    "\n",
    "6. **Chi-Square Test:**\n",
    "   - Used for categorical data, the chi-square test assesses the independence of variables. A low p-value in a chi-square test suggests that the observed distribution is significantly different from the expected distribution.\n",
    "\n",
    "7. **ANOVA (Analysis of Variance):**\n",
    "   - For comparing means across multiple groups, ANOVA tests the statistical significance. A low p-value indicates that there is a significant difference between at least two groups.\n",
    "\n",
    "It's essential to choose the criteria that align with the goals of the analysis and the nature of the data. The interpretation of statistical importance should also consider practical significance and contextual relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What are the applications of long-tail distributions?\n",
    "\n",
    "A long-tailed distribution is when the tail progressively diminishes as the\n",
    "curve progresses to the finish. The usage of long-tailed distributions is\n",
    "exemplified by the Pareto principle and the product sales distribution, and\n",
    "it's also famous for classification and regression difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question \"What are the applications of long-tail distributions?\", we need to understand the concept of long-tail distributions first.\n",
    "\n",
    "A long-tail distribution is a statistical distribution where the frequency of occurrence of values decreases gradually as the value increases. In other words, it has a long tail of rare or infrequent events.\n",
    "\n",
    "Here are some applications of long-tail distributions:\n",
    "\n",
    "1. **Economics and Business:** Long-tail distributions are commonly observed in economic and business contexts. For example, in e-commerce, the sales of popular products follow a power-law distribution, where a few products have high sales volume (head) and many products have low sales volume (tail). Understanding the long-tail distribution can help businesses optimize their product offerings and marketing strategies.\n",
    "\n",
    "2. **Internet and Digital Media:** Long-tail distributions are prevalent in internet-based platforms and digital media. Online marketplaces, streaming services, and content platforms often have a vast catalog of products or content, with a few popular items dominating the head and a long tail of niche or less popular items. Analyzing the long-tail distribution can provide insights into user preferences, content recommendations, and personalized experiences.\n",
    "\n",
    "3. **Recommendation Systems:** Long-tail distributions play a crucial role in recommendation systems. By considering the long tail of user preferences and item popularity, recommendation algorithms can suggest personalized and diverse recommendations to users. This helps in discovering niche or less-known items and avoids the \"filter bubble\" effect.\n",
    "\n",
    "4. **Search Engines:** Long-tail distributions are relevant in search engine optimization (SEO) and search result ranking. Optimizing for long-tail keywords and understanding the distribution of search queries can help websites and content creators attract targeted traffic and improve visibility in search engine results.\n",
    "\n",
    "5. **Data Analysis and Modeling:** Long-tail distributions are encountered in various data analysis and modeling tasks. They can affect the choice of statistical models, sampling strategies, and outlier detection methods. Understanding the long-tail nature of the data can lead to more accurate and robust analyses.\n",
    "\n",
    "These are just a few examples of the applications of long-tail distributions. The specific applications may vary depending on the domain and context of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What is the definition of the central limit theorem, and what is its application?\n",
    "The central limit theorem asserts that when the sample size changes without\n",
    "changing the form of the population distribution, the normal distribution is\n",
    "obtained. The central limit theorem is crucial since it is commonly utilized\n",
    "in hypothesis testing and precisely calculating confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central limit theorem (CLT) states that when independent random variables are added, their sum tends toward a normal distribution, regardless of the shape of the original distribution. In other words, as the sample size increases, the distribution of the sample mean approaches a normal distribution.\n",
    "\n",
    "The central limit theorem has several applications in statistics and data analysis:\n",
    "\n",
    "1. **Hypothesis Testing:** The CLT is used to perform hypothesis tests, where the null hypothesis assumes a specific population distribution. By calculating the sample mean and standard deviation, we can determine the probability of observing a sample mean as extreme as the one obtained, assuming the null hypothesis is true.\n",
    "\n",
    "2. **Confidence Intervals:** The CLT is used to construct confidence intervals for population parameters. By calculating the sample mean and standard deviation, we can estimate the range within which the true population mean is likely to fall with a certain level of confidence.\n",
    "\n",
    "3. **Sampling Distributions:** The CLT helps in understanding the properties of sampling distributions. It allows us to make inferences about the population based on a sample, by providing insights into the distribution of sample statistics such as the sample mean.\n",
    "\n",
    "4. **Estimation and Prediction:** The CLT is used in estimating population parameters and making predictions. By assuming that the sample mean follows a normal distribution, we can use statistical techniques to estimate unknown population parameters and make predictions about future observations.\n",
    "\n",
    "5. **Statistical Modeling:** The CLT is fundamental in statistical modeling. It forms the basis for many statistical techniques, such as linear regression, where the assumptions of normality and independence are often required.\n",
    "\n",
    "Overall, the central limit theorem is a fundamental concept in statistics that allows us to make inferences about populations based on sample data. It provides a bridge between the sample and the population, enabling us to draw meaningful conclusions and make statistical decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. In statistics, what do we understand by observational and experimental data?\n",
    "Data from observational studies, in which variables are examined to see a\n",
    "link, is referred to as observational data. Experimental data comes from\n",
    "investigations in which specific factors are kept constant to examine any\n",
    "disparity in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observational data refers to data collected from observational studies, where variables are examined to observe any potential relationships or correlations. In observational studies, researchers do not actively manipulate or control the variables but rather observe and collect data as it naturally occurs.\n",
    "\n",
    "On the other hand, experimental data is obtained from controlled experiments where researchers actively manipulate and control the variables of interest. In experimental studies, participants or subjects are assigned to different groups or conditions, and the effects of the manipulated variables are observed and measured.\n",
    "\n",
    "The main difference between observational and experimental data lies in the level of control and manipulation of variables. Observational data is collected in real-world settings without intervention, while experimental data is collected under controlled conditions with intentional manipulation of variables. Experimental studies allow for stronger causal inferences, while observational studies can provide valuable insights into naturally occurring relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. What does mean imputation for missing data means? What are its disadvantages?\n",
    "Mean imputation is a seldom-used technique that involves replacing null\n",
    "values in a dataset with the data's mean. It's a terrible approach since it\n",
    "removes any accountability for feature correlation. This also indicates that\n",
    "the data will have low variance and a higher bias, reducing the model's\n",
    "accuracy and narrowing confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean imputation for missing data refers to the technique of replacing missing values in a dataset with the mean value of the available data. It is a simple and commonly used method for handling missing data.\n",
    "\n",
    "However, mean imputation has several disadvantages:\n",
    "\n",
    "1. **Loss of Variability:** By replacing missing values with the mean, the imputed dataset loses the variability that was present in the original data. This can lead to an underestimation of the true variability in the dataset.\n",
    "\n",
    "2. **Bias in Estimates:** Mean imputation introduces bias in the estimates of the mean and other statistics. The imputed values are artificially inflated or deflated towards the mean, which can distort the true distribution of the data.\n",
    "\n",
    "3. **Underestimation of Standard Errors:** Mean imputation tends to underestimate the standard errors of the estimates. This can lead to incorrect hypothesis testing and confidence intervals that are too narrow.\n",
    "\n",
    "4. **Distortion of Relationships:** Mean imputation ignores the relationships between variables. It assumes that missing values are missing completely at random (MCAR), which may not be true in practice. This can lead to biased estimates of the relationships between variables.\n",
    "\n",
    "5. **Inflation of Correlations:** Mean imputation can artificially inflate the correlations between variables. This is because the imputed values introduce a common mean value, which can increase the apparent correlation between variables.\n",
    "\n",
    "6. **Inaccurate Model Performance:** Mean imputation can lead to inaccurate model performance. The imputed values may not reflect the true values of the missing data, which can affect the model's ability to make accurate predictions.\n",
    "\n",
    "Overall, mean imputation is a simple and convenient method for handling missing data, but it has several disadvantages that can affect the validity and accuracy of the analysis. It is important to consider these limitations and explore alternative methods for handling missing data, such as multiple imputation or model-based imputation, which can provide more reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. What is the definition of an outlier, and how do we recognize one in a dataset?\n",
    "Data points that differ significantly from the rest of the dataset are called\n",
    "outliers. Depending on the learning process, an outlier can significantly\n",
    "reduce a model's accuracy and efficiency.\n",
    "Two strategies are used to identify outliers:\n",
    "* Interquartile range (IQR)\n",
    "* Standard deviation/z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An outlier is a data point that significantly deviates from the rest of the dataset. It is an observation that lies an abnormal distance away from other values. Outliers can occur due to various reasons such as measurement errors, data entry errors, or genuine rare events.\n",
    "\n",
    "There are several strategies to recognize outliers in a dataset. Two commonly used methods are:\n",
    "\n",
    "1. **Interquartile Range (IQR):** The IQR is a measure of statistical dispersion that represents the range between the first quartile (25th percentile) and the third quartile (75th percentile) of the data. In this method, any data point that falls below the first quartile minus 1.5 times the IQR or above the third quartile plus 1.5 times the IQR is considered an outlier.\n",
    "\n",
    "2. **Z-Score:** The z-score is a measure of how many standard deviations a data point is away from the mean of the dataset. In this method, data points that have a z-score greater than a certain threshold (e.g., 2 or 3) are considered outliers.\n",
    "\n",
    "By applying these methods, we can identify and flag potential outliers in a dataset. However, it is important to carefully analyze and interpret outliers, as they can have a significant impact on statistical analyses and machine learning models. Outliers should be examined to determine whether they are genuine anomalies or data errors, and appropriate actions can be taken based on the specific context and goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. In statistics, how are missing data treated?\n",
    "In Statistics, there are several options for dealing with missing data:\n",
    "* Missing values prediction\n",
    "* Individual (one-of-a-kind) value assignment\n",
    "* Rows with missing data should be deleted\n",
    "* Imputation by use of a mean or median value\n",
    "* Using random forests to help fill in the blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, missing data can be treated in several ways:\n",
    "\n",
    "1. **Missing Values Prediction:** This approach involves using statistical models or machine learning algorithms to predict the missing values based on the available data. The predicted values can then be used to fill in the missing data.\n",
    "\n",
    "2. **Individual (One-of-a-Kind) Value Assignment:** In some cases, missing values can be assigned unique values that indicate their absence. For example, a missing value in a categorical variable can be assigned a special category like \"Unknown\" or \"Not Applicable\".\n",
    "\n",
    "3. **Deletion of Rows with Missing Data:** If the missing data is limited to a small number of observations, one option is to simply delete the rows with missing values. However, this approach should be used with caution as it can lead to loss of valuable information and potential bias in the analysis.\n",
    "\n",
    "4. **Imputation using Mean or Median:** Another common approach is to impute the missing values with the mean or median value of the variable. This method assumes that the missing values are missing at random and that the mean or median is a reasonable estimate for the missing values.\n",
    "\n",
    "These are just a few examples of how missing data can be treated in statistics. The choice of method depends on the nature of the data, the extent of missingness, and the specific analysis or modeling goals. It is important to carefully consider the implications of each method and select the most appropriate approach for the given situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. What is exploratory data analysis, and how does it differ from other types of data analysis?\n",
    "Investigating data to comprehend it better is known as exploratory data\n",
    "analysis. Initial investigations are carried out to identify patterns, detect\n",
    "anomalies, test hypotheses, and confirm correct assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis (EDA) is the process of investigating and analyzing data to gain insights, discover patterns, detect anomalies, test hypotheses, and confirm assumptions. It involves visualizing and summarizing the data using various statistical and graphical techniques.\n",
    "\n",
    "EDA differs from other types of data analysis in several ways:\n",
    "\n",
    "1. **Objective:** The primary objective of EDA is to understand the data and uncover interesting patterns or relationships. It focuses on exploring the data without making any specific assumptions or hypotheses.\n",
    "\n",
    "2. **Exploratory Nature:** EDA is an iterative and open-ended process. It involves exploring the data from different angles, generating hypotheses, and refining the analysis based on the insights gained. It allows for flexibility and creativity in the analysis.\n",
    "\n",
    "3. **Descriptive Statistics:** EDA relies heavily on descriptive statistics to summarize and describe the data. It involves calculating measures of central tendency, dispersion, and correlation to understand the distribution and relationships within the data.\n",
    "\n",
    "4. **Visualization Techniques:** EDA emphasizes the use of visualizations to explore and present the data. It includes techniques such as histograms, scatter plots, box plots, and heatmaps to visualize the distribution, relationships, and patterns in the data.\n",
    "\n",
    "5. **Data Cleaning and Preprocessing:** EDA often involves data cleaning and preprocessing steps to handle missing values, outliers, and inconsistencies in the data. These steps are crucial for ensuring the quality and reliability of the analysis.\n",
    "\n",
    "6. **Hypothesis Generation:** EDA is often used to generate hypotheses or insights that can be further tested using formal statistical methods or machine learning algorithms. It provides a foundation for more advanced analyses and modeling.\n",
    "\n",
    "Overall, exploratory data analysis is a crucial step in the data analysis process. It helps in understanding the data, identifying potential issues, and guiding further analysis. EDA provides valuable insights that can inform decision-making, problem-solving, and the development of more sophisticated analytical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. What is selection bias, and what does it imply?\n",
    "The phenomenon of selection bias refers to the non-random selection of\n",
    "individual or grouped data to undertake analysis and better understand\n",
    "model functionality. If proper randomization is not performed, the sample\n",
    "will not correctly represent the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection bias refers to the systematic error or distortion that occurs when the selection of individuals or groups for analysis is not random or representative of the target population. It implies that the sample used for analysis does not accurately reflect the characteristics or distribution of the population, leading to biased and potentially misleading results.\n",
    "\n",
    "Selection bias can arise in various ways, such as:\n",
    "\n",
    "1. **Non-Random Sampling:** When the selection of individuals or groups is not random, certain segments of the population may be overrepresented or underrepresented in the sample. This can result in a skewed or unrepresentative sample.\n",
    "\n",
    "2. **Self-Selection:** When individuals or groups self-select to participate in a study or survey, it can introduce bias if the characteristics of those who choose to participate differ from those who do not. This can lead to a sample that is not representative of the target population.\n",
    "\n",
    "3. **Survivorship Bias:** When only a subset of individuals or groups is included in the analysis due to the exclusion of certain cases or data points, survivorship bias can occur. This can distort the results by excluding important information or skewing the observed patterns.\n",
    "\n",
    "4. **Sampling Bias:** If the sampling method used is biased or flawed, it can result in a sample that does not accurately represent the population. This can introduce systematic errors and affect the generalizability of the findings.\n",
    "\n",
    "The implications of selection bias are significant. It can lead to incorrect conclusions, invalid inferences, and flawed decision-making. The results obtained from a biased sample may not be applicable or generalizable to the larger population, limiting the external validity of the study. It is crucial to minimize and account for selection bias in research and analysis to ensure the accuracy and reliability of the findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. What are the many kinds of statistical selection bias?\n",
    "As indicated below, there are different kinds of selection bias:\n",
    "* Protopathic bias\n",
    "* Observer selection\n",
    "* Attrition\n",
    "* Sampling bias\n",
    "* Time intervals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different kinds of statistical selection bias, including:\n",
    "\n",
    "1. Protopathic Bias: This bias occurs when the exposure or outcome of interest is measured after the occurrence of an intermediate event. It can lead to a distorted association between the exposure and outcome due to the timing of measurement.\n",
    "\n",
    "2. Observer Selection Bias: This bias occurs when the selection of observers or researchers is not random or representative. It can introduce bias in the data collection process and affect the validity of the results.\n",
    "\n",
    "3. Attrition Bias: This bias occurs when there is a differential loss of participants or data during a study. It can lead to a biased sample and affect the generalizability of the findings.\n",
    "\n",
    "4. Sampling Bias: This bias occurs when the sampling method used is not random or representative of the target population. It can result in a sample that does not accurately reflect the characteristics or distribution of the population.\n",
    "\n",
    "These are just a few examples of statistical selection bias. It is important to be aware of these biases and take appropriate measures to minimize their impact on the analysis and interpretation of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. What is the definition of an inlier?\n",
    "An inlier is a data point on the same level as the rest of the dataset. As\n",
    "opposed to an outlier, finding an inlier in a dataset is more challenging\n",
    "because it requires external data. Outliers diminish model accuracy, and\n",
    "inliers do the same. As a result, they're also eliminated if found in the data.\n",
    "This is primarily done to ensure that the model is always accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inlier is a data point that is considered to be within the normal range or distribution of the dataset. Unlike an outlier, which deviates significantly from the rest of the data, an inlier is on the same level as the majority of the dataset. Identifying inliers can be challenging as it often requires external data or context. Inliers can also impact model accuracy, similar to outliers, and may be eliminated from the data to ensure the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Describe a situation in which the median is superior to the mean.\n",
    "When some outliers might skew data either favorably or negatively, the\n",
    "median is preferable since it offers an appropriate assessment in this\n",
    "instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In situations where the data contains outliers that can significantly skew the distribution, the median is often considered superior to the mean. The median is less affected by extreme values and provides a more robust measure of central tendency. This is particularly useful when the goal is to understand the typical or representative value of the data, rather than being influenced by outliers. By using the median, we can obtain a more accurate assessment of the data in such instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Could you provide an example of a root cause analysis?\n",
    "As the name implies, root cause analysis is a problem-solving technique\n",
    "that identifies the problem's fundamental cause. For instance, if a city's\n",
    "greater crime rate is directly linked to higher red-colored shirt sales, this\n",
    "indicates that the two variables are positively related. However, this does\n",
    "not imply that one is responsible for the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Here's an example of a root cause analysis:\n",
    "\n",
    "Let's say a manufacturing company is experiencing a high rate of product defects. The company decides to conduct a root cause analysis to identify the underlying cause of the issue.\n",
    "\n",
    "1. Problem Statement: The problem is the high rate of product defects.\n",
    "\n",
    "2. Data Collection: The company collects data on various factors that could potentially contribute to the defects, such as raw material quality, machine settings, operator skills, and production process parameters.\n",
    "\n",
    "3. Analysis: The company analyzes the collected data and identifies that the machine settings and operator skills are the two main factors that could be causing the defects.\n",
    "\n",
    "4. Root Cause Identification: Further investigation reveals that the machine settings are not properly calibrated, leading to inconsistent product quality. Additionally, some operators lack proper training and are not following the standard operating procedures.\n",
    "\n",
    "5. Solution Implementation: Based on the root causes identified, the company takes corrective actions. They recalibrate the machines to ensure consistent product quality and provide training to the operators to improve their skills and adherence to standard procedures.\n",
    "\n",
    "6. Monitoring and Evaluation: The company monitors the production process after implementing the solutions to ensure that the defect rate decreases. They evaluate the effectiveness of the corrective actions by comparing the defect rates before and after the changes.\n",
    "\n",
    "By conducting a root cause analysis, the manufacturing company was able to identify the underlying issues causing the high rate of product defects and implement targeted solutions to address them. This approach helps in resolving the problem at its source, leading to improved product quality and customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. What does the term \"six sigma\" mean?\n",
    "Six sigma is a quality assurance approach frequently used in statistics to\n",
    "enhance procedures and functionality while working with data. A process is\n",
    "called six sigma when 99.99966 percent of the model's outputs are defect-\n",
    "free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six sigma is a quality assurance approach frequently used in statistics to enhance procedures and functionality while working with data. A process is called six sigma when 99.99966 percent of the model's outputs are defect-free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. What is the definition of DOE?\n",
    "In statistics, DOE stands for \"Design of Experiments.\" The task design\n",
    "specifies the data and varies when the independent input factors change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, DOE stands for \"Design of Experiments.\" It is a systematic approach used to plan, conduct, analyze, and interpret experiments. The goal of DOE is to optimize the process or system being studied by identifying the key factors that influence the response variable and determining the optimal settings for these factors. DOE involves carefully designing the experiment, controlling the factors being studied, and collecting and analyzing the data to draw meaningful conclusions. By using DOE, researchers can efficiently explore the effects of different factors and their interactions, leading to improved understanding and optimization of the process or system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Which of the following data types does not have a log-normal or Gaussian distribution?\n",
    "There are no log-normal or Gaussian distributions in exponential\n",
    "distributions, and in reality, these distributions do not exist for categorical\n",
    "data of any kind. Typical examples are the duration of a phone call, the time\n",
    "until the next earthquake, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data type that does not have a log-normal or Gaussian distribution is categorical data. Categorical data does not follow a continuous distribution and therefore cannot be described by a log-normal or Gaussian distribution. Examples of categorical data include the type of car (sedan, SUV, truck), the color of a shirt (red, blue, green), or the category of a product (electronics, clothing, furniture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. What does the five-number summary mean in Statistics?\n",
    "As seen below, the five-number summary is a measure of five entities that\n",
    "encompass the complete range of data:\n",
    "* Upper quartile (Q3)\n",
    "* High extreme (Max)\n",
    "* Median\n",
    "* Low extreme (Min)\n",
    "* The first quartile (Q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five-number summary in statistics provides a concise summary of the distribution of a dataset. It consists of five key values: the minimum (Low extreme), the first quartile (Q1), the median, the third quartile (Q3), and the maximum (High extreme). These values help to understand the spread, central tendency, and skewness of the data. The five-number summary is often used in box plots to visualize the distribution of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. What is the definition of the Pareto principle?\n",
    "The Pareto principle, commonly known as the 80/20 rule, states that 80% of\n",
    "the results come from 20% of the causes in a given experiment. The\n",
    "observation that 80 percent of peas originate from 20% of pea plants on a\n",
    "farm is a basic example of the Pareto principle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pareto principle, commonly known as the 80/20 rule, states that 80% of the results come from 20% of the causes in a given experiment. The observation that 80 percent of peas originate from 20% of pea plants on a farm is a basic example of the Pareto principle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
