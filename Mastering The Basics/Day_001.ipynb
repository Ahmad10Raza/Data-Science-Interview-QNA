{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What exactly does the term \"Data Science\" mean?\n",
    "\n",
    "Data Science is an interdisciplinary discipline that encompasses a variety of\n",
    "scientific procedures, algorithms, tools, and machine learning algorithms\n",
    "that work together to uncover common patterns and gain useful insights\n",
    "from raw input data using statistical and mathematical analysis.\n",
    "Gathering business needs and related data is the first step; data cleansing,\n",
    "data staging, data warehousing, and data architecture are all procedures in\n",
    "the data acquisition process. Exploring, mining, and analyzing data are all\n",
    "tasks that data processing does, and the results may then be utilized to\n",
    "provide a summary of the data's insights.\n",
    "Following the exploratory phases, the cleansed data is exposed to many\n",
    "algorithms, such as predictive analysis, regression, text mining, pattern\n",
    "recognition, and so on, depending on the needs. In the final last stage, the\n",
    "outcomes are aesthetically appealingly when conveyed to the business. This\n",
    "is where the ability to see data, report on it, and use other business\n",
    "intelligence tools come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Data Science\" is a multidisciplinary field that involves using scientific methods, processes, algorithms, and systems to extract insights and knowledge from structured and unstructured data. It combines expertise from various domains, including statistics, mathematics, computer science, and domain-specific knowledge, to analyze and interpret complex data sets.\n",
    "\n",
    "Key components of data science include:\n",
    "\n",
    "1. **Data Collection and Storage:** Gathering and storing relevant data from various sources.\n",
    "\n",
    "2. **Data Cleaning and Preprocessing:** Handling missing values, outliers, and formatting data for analysis.\n",
    "\n",
    "3. **Exploratory Data Analysis (EDA):** Exploring and visualizing data to understand patterns, trends, and relationships.\n",
    "\n",
    "4. **Feature Engineering:** Creating new features or transforming existing ones to improve model performance.\n",
    "\n",
    "5. **Machine Learning:** Applying algorithms to build predictive models or make data-driven decisions.\n",
    "\n",
    "6. **Model Evaluation and Validation:** Assessing the performance of models and ensuring they generalize well to new data.\n",
    "\n",
    "7. **Deployment:** Implementing models into production systems for real-world use.\n",
    "\n",
    "8. **Communication of Results:** Effectively communicating findings to both technical and non-technical stakeholders.\n",
    "\n",
    "Data scientists often use programming languages such as Python or R, along with specialized tools and frameworks, to perform these tasks. The ultimate goal of data science is to derive valuable insights, inform decision-making, and solve complex problems within various industries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What is the difference between data science and data analytics?\n",
    "Data science is altering data using a variety of technical analysis approaches\n",
    "to derive useful insights that data analysts may apply to their business\n",
    "scenarios.\n",
    "Data analytics is concerned with verifying current hypotheses and facts and\n",
    "answering questions for a more efficient and successful business decision-\n",
    "making process.\n",
    "Data Science fosters innovation by providing answers to questions that help\n",
    "people make connections and solve challenges in the future. Data analytics\n",
    "is concerned with removing current meaning from past context, whereas\n",
    "data science is concerned with predictive modelling.\n",
    "Data science is a wide topic that employs a variety of mathematical and\n",
    "scientific tools and methods to solve complicated issues. In contrast, data\n",
    "analytics is a more focused area that employs fewer statistical and\n",
    "visualization techniques to solve particular problems.\n",
    "\n",
    "\n",
    " Let's represent the differences between data science and data analytics using a tabular format:\n",
    "\n",
    "| Aspect                     | Data Science                                      | Data Analytics                                    |\n",
    "|----------------------------|---------------------------------------------------|---------------------------------------------------|\n",
    "| **Scope and Purpose**       | Broader field involving descriptive, predictive, and prescriptive analytics. | Focused on processing and analyzing historical data.|\n",
    "| **Techniques and Methods**  | Encompasses machine learning, statistical modeling, data mining, and advanced analytics. | Primarily involves basic statistical analysis, data cleaning, and visualization techniques.|\n",
    "| **Goal and Application**    | Aims to uncover hidden patterns, make predictions, and provide actionable insights for complex problems. | Focuses on examining historical data to identify trends, analyze the effects of decisions, and support business strategies.|\n",
    "| **Tools and Technologies**  | Utilizes programming languages (e.g., Python, R), machine learning frameworks, and big data technologies. | Commonly uses tools like Excel, SQL, and visualization tools (e.g., Tableau, Power BI).|\n",
    "\n",
    "This table provides a concise overview of the key distinctions between data science and data analytics across various aspects. If you have any specific questions or if you'd like to explore a particular aspect further, feel free to let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What are some of the strategies utilized for sampling? What is the major advantage of sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling?\n",
    "\n",
    "**Sampling** is the process of selecting a subset of elements from a larger population to make inferences or draw conclusions about the entire population. It is a common practice in statistics and data analysis when it is impractical or too costly to gather data from an entire population.\n",
    "\n",
    "Here's a simple example to illustrate the concept of sampling:\n",
    "\n",
    "**Example: Population of Students**\n",
    "\n",
    "Imagine you want to determine the average height of all students in a large university, which has thousands of students. Instead of measuring the height of every single student (which could be time-consuming and resource-intensive), you decide to use sampling.\n",
    "\n",
    "1. **Population:** All students in the university.\n",
    "\n",
    "2. **Sampling Frame:** A list of all students' names or ID numbers.\n",
    "\n",
    "3. **Sample:** You randomly select 100 students from the sampling frame.\n",
    "\n",
    "4. **Data Collection:** Measure the height of the selected 100 students.\n",
    "\n",
    "5. **Inference:** Use the average height of the sampled students to make an estimate or inference about the average height of all students in the university.\n",
    "\n",
    "In this example, the 100 students you selected represent a sample from the entire population of students. The key is to ensure that the sampling process is random or unbiased to make valid inferences about the population based on the characteristics observed in the sample.\n",
    "\n",
    "Different sampling methods exist, such as simple random sampling, stratified sampling, and cluster sampling, each with its own advantages and use cases. The choice of sampling method depends on the research question, available resources, and the nature of the population under study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What are some of the strategies utilized for sampling? What is the major advantage of sampling?\n",
    "Data analysis cannot be done on an entire amount of data at a time,\n",
    "especially when it concerns bigger datasets. It becomes important to obtain\n",
    "data samples that can represent the full population and then analyse it.\n",
    "While doing this, it is vital to properly choose sample data out of the\n",
    "enormous data that represents the complete dataset.\n",
    "There are two types of sampling procedures depending on the engagement\n",
    "of statistics, they are:\n",
    "\n",
    "* Non-Probability sampling techniques: Convenience sampling,\n",
    "Quota sampling, snowball sampling, etc.\n",
    "* Probability sampling techniques: Simple random sampling,\n",
    "clustered sampling, stratified sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several strategies utilized for sampling, each with its own advantages and use cases. Here are some common sampling strategies:\n",
    "\n",
    "1. **Simple Random Sampling:**\n",
    "   - **Description:** Each individual in the population has an equal chance of being selected.\n",
    "   - **Advantage:** Easy to implement, unbiased representation of the population.\n",
    "\n",
    "2. **Stratified Sampling:**\n",
    "   - **Description:** The population is divided into subgroups (strata), and random samples are taken from each stratum.\n",
    "   - **Advantage:** Ensures representation from all subgroups, especially useful when the population has distinct characteristics.\n",
    "\n",
    "3. **Systematic Sampling:**\n",
    "   - **Description:** Individuals are selected at regular intervals from a list after a random start.\n",
    "   - **Advantage:** Simplicity and efficiency, especially when a complete list of the population is available.\n",
    "\n",
    "4. **Cluster Sampling:**\n",
    "   - **Description:** The population is divided into clusters, and entire clusters are randomly selected.\n",
    "   - **Advantage:** Useful when it is more practical to sample groups (clusters) rather than individuals, especially in geographically dispersed populations.\n",
    "\n",
    "5. **Convenience Sampling:**\n",
    "   - **Description:** Individuals are chosen based on their accessibility and convenience.\n",
    "   - **Advantage:** Quick and easy, but may not be representative of the entire population.\n",
    "\n",
    "6. **Snowball Sampling:**\n",
    "   - **Description:** Initial participants are used to refer and recruit additional participants.\n",
    "   - **Advantage:** Useful for hard-to-reach populations where members are interconnected.\n",
    "\n",
    "**Major Advantage of Sampling:**\n",
    "\n",
    "The primary advantage of sampling is **efficiency.** Instead of collecting data from an entire population, which can be time-consuming, resource-intensive, and sometimes impractical, sampling allows researchers to study a subset of the population and make inferences about the entire population based on the characteristics observed in the sample.\n",
    "\n",
    "Other advantages include cost-effectiveness, feasibility, and the ability to obtain results more quickly. However, it's crucial to ensure that the sampling method used is appropriate for the research question and that the sample is representative to make valid inferences about the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. List down the criteria for Overfitting and Underfitting\n",
    "* **Overfitting:** The model works well just on the sample training data. Any\n",
    "new data is supplied as input to the model fails to generate any result. These\n",
    "situations emerge owing to low bias and large variation in the model.\n",
    "Decision trees are usually prone to overfitting.\n",
    "* **Underfitting:** Here, the model is very simple in that it cannot find the proper\n",
    "connection in the data, and consequently, it does not perform well on the\n",
    "test data. This might arise owing to excessive bias and low variance.\n",
    "Underfitting is more common in linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criteria for Overfitting:**\n",
    "\n",
    "1. **Training Performance:**\n",
    "   - Overfitting often results in very high accuracy on the training data.\n",
    "\n",
    "2. **Validation Performance:**\n",
    "   - The model performs poorly on new, unseen data or a validation dataset.\n",
    "\n",
    "3. **Complexity:**\n",
    "   - Overfit models are excessively complex, capturing noise in the training data as if it were a meaningful pattern.\n",
    "\n",
    "4. **Generalization Gap:**\n",
    "   - There is a significant difference between the performance on the training set and the validation/test set.\n",
    "\n",
    "5. **Model Complexity vs. Data Size:**\n",
    "   - Overfitting is more likely when the model is too complex relative to the size of the training data.\n",
    "\n",
    "**Criteria for Underfitting:**\n",
    "\n",
    "1. **Training Performance:**\n",
    "   - The model performs poorly on the training data, indicating a failure to capture the underlying patterns.\n",
    "\n",
    "2. **Validation Performance:**\n",
    "   - Similar poor performance is observed on new, unseen data or a validation dataset.\n",
    "\n",
    "3. **Simplicity:**\n",
    "   - The model is too simple to represent the underlying patterns in the data.\n",
    "\n",
    "4. **Generalization Gap:**\n",
    "   - There is little difference between the performance on the training set and the validation/test set, but both are poor.\n",
    "\n",
    "5. **Model Complexity vs. Data Size:**\n",
    "   - Underfitting can occur when the model is too simple, regardless of the size of the training data.\n",
    "\n",
    "Balancing between underfitting and overfitting involves finding an optimal model complexity that captures the underlying patterns in the data without fitting noise or being too simplistic. Regularization techniques and proper model evaluation are crucial for addressing these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Distinguish between data in long and wide formats.\n",
    "Data in a long format\n",
    "* Each row of the data reflects a subject's one-time information.\n",
    "Each subject's data would be organized in different/multiple\n",
    "rows.\n",
    "* When viewing rows as groupings, the data may be identified.\n",
    "* This data format is commonly used in R analysis and for writing\n",
    "to log files at the end of each experiment.\n",
    "\n",
    "Data in a Wide Format\n",
    "* The repeated replies of a subject are divided into various\n",
    "columns in this example.\n",
    "* By viewing columns as groups, the data may be identified.\n",
    "* This data format is most widely used in stats programs for\n",
    "repeated measures ANOVAs and is seldom utilized in R\n",
    "analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Distinguishing Between Long and Wide Formats:**\n",
    "\n",
    "1. **Long Format:**\n",
    "   - **Structure:**\n",
    "     - Each row represents a unique observation or data point.\n",
    "     - Multiple columns may represent different variables, including an identifier and a value column.\n",
    "   - **Use Case:**\n",
    "     - Suitable for scenarios where you have repeated measurements on the same subjects or entities.\n",
    "     - Often used in relational databases.\n",
    "   - **Example:**\n",
    "     ```\n",
    "     | SubjectID | Variable   | Value |\n",
    "     |-----------|------------|-------|\n",
    "     | 1         | Age        | 25    |\n",
    "     | 1         | Height     | 170   |\n",
    "     | 1         | Weight     | 65    |\n",
    "     | 2         | Age        | 30    |\n",
    "     | 2         | Height     | 160   |\n",
    "     ```\n",
    "\n",
    "2. **Wide Format:**\n",
    "   - **Structure:**\n",
    "     - Each row represents a unique subject or entity.\n",
    "     - Variables are spread across columns, with each column representing a specific variable.\n",
    "   - **Use Case:**\n",
    "     - Convenient for analyses and visualizations that require a simpler, tabular structure.\n",
    "     - Often used when data is collected at a single time point per subject.\n",
    "   - **Example:**\n",
    "     ```\n",
    "     | SubjectID | Age | Height | Weight |\n",
    "     |-----------|-----|--------|--------|\n",
    "     | 1         | 25  | 170    | 65     |\n",
    "     | 2         | 30  | 160    | -      |\n",
    "     ```\n",
    "\n",
    "3. **Key Differences:**\n",
    "   - **Length of Data:**\n",
    "     - Long format tends to have more rows, while wide format often results in wider datasets.\n",
    "   - **Readability:**\n",
    "     - Long format is more compact, while wide format may be easier to read for a small number of variables.\n",
    "   - **Analysis:**\n",
    "     - Long format is conducive to certain types of analyses (e.g., repeated measures), while wide format may be more suitable for others (e.g., summary statistics).\n",
    "\n",
    "Choosing between long and wide formats depends on the nature of your data, the analysis you plan to perform, and the requirements of the tools or methods you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. What is the difference between Eigenvectors and Eigenvalues?\n",
    "Eigenvectors are column vectors of unit vectors with a length/magnitude of\n",
    "1; they are also known as right vectors. Eigenvalues are coefficients applied\n",
    "to eigenvectors to varying length or magnitude values. Eigen decomposition\n",
    "is the process of breaking down a matrix into Eigenvectors and\n",
    "Eigenvalues. These are then utilized in machine learning approaches such\n",
    "as PCA (Principal Component Analysis) to extract useful information from\n",
    "a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eigenvectors and eigenvalues** are concepts from linear algebra, often encountered in various fields, including physics, computer science, and data analysis. Here's a concise distinction between the two:\n",
    "\n",
    "1. **Eigenvalues:**\n",
    "   - **Definition:** Eigenvalues are scalar values that represent the factor by which a matrix scales or compresses space in a linear transformation.\n",
    "   - **Notation:** Denoted by λ (lambda) in mathematical expressions.\n",
    "   - **Calculation:** For a square matrix A, eigenvalues (λ) are found by solving the characteristic equation |A - λI| = 0, where I is the identity matrix.\n",
    "\n",
    "2. **Eigenvectors:**\n",
    "   - **Definition:** Eigenvectors are non-zero vectors that remain in the same direction (possibly scaled) when a linear transformation is applied using a specific eigenvalue.\n",
    "   - **Notation:** Denoted by v in mathematical expressions.\n",
    "   - **Calculation:** For a given eigenvalue λ, eigenvectors (v) are found by solving the system of linear equations (A - λI)v = 0, where A is the matrix and I is the identity matrix.\n",
    "\n",
    "**Key Differences:**\n",
    "- **Nature:** Eigenvalues are scalar values, whereas eigenvectors are vectors.\n",
    "- **Representation:** Eigenvalues represent the scaling factor of the transformation, while eigenvectors represent the directions that remain unchanged.\n",
    "- **Calculation:** Eigenvalues are obtained by solving the characteristic equation, while eigenvectors are obtained by solving a system of linear equations for each eigenvalue.\n",
    "\n",
    "In practical terms, eigenvectors and eigenvalues are often used in techniques like Principal Component Analysis (PCA) in data science and machine learning, where they help in understanding the most important directions and magnitudes of variability within a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. What does it mean to have high and low p-values?\n",
    "A p-value measures the possibility of getting outcomes equal to or greater\n",
    "than those obtained under a certain hypothesis, provided the null hypothesis\n",
    "is true. This indicates the likelihood that the observed discrepancy happened\n",
    "by coincidence.\n",
    "\n",
    "When the p-value is less than 0.05, we say have a low p-value, the null\n",
    "hypothesis may be rejected, and the data is unlikely to be true null. A high\n",
    "p-value indicates the strength in support of the null hypothesis, i.e., values\n",
    "greater than 0.05, indicating that the data is true null. The hypothesis can go\n",
    "either way with a p-value of 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistical hypothesis testing, the p-value is a measure that helps assess the evidence against a null hypothesis. Here's what it means to have high and low p-values:\n",
    "\n",
    "1. **Low P-value:**\n",
    "   - **Definition:** A low p-value (typically below a chosen significance level, often 0.05) indicates strong evidence against the null hypothesis.\n",
    "   - **Interpretation:** It suggests that the observed data is unlikely to have occurred if the null hypothesis were true. In other words, a low p-value suggests that there is a significant effect or difference.\n",
    "\n",
    "2. **High P-value:**\n",
    "   - **Definition:** A high p-value (above the chosen significance level) suggests weak evidence against the null hypothesis.\n",
    "   - **Interpretation:** It implies that the observed data is likely to occur even if the null hypothesis were true. In this case, there is not enough evidence to reject the null hypothesis.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- **Low p-value (e.g., p < 0.05):** Indicates evidence to reject the null hypothesis. Results are considered statistically significant, suggesting that there is a real effect or difference.\n",
    "\n",
    "- **High p-value (e.g., p > 0.05):** Indicates insufficient evidence to reject the null hypothesis. Results are not considered statistically significant, suggesting that observed effects may be due to chance, and the null hypothesis is not necessarily false.\n",
    "\n",
    "It's crucial to choose a significance level (often denoted as alpha, α) before conducting hypothesis testing. Commonly used values for alpha are 0.05, 0.01, or 0.10. The chosen significance level represents the threshold beyond which a p-value is considered either low or high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. When to do re-sampling?\n",
    "Re-sampling is a data sampling procedure that improves accuracy and\n",
    "quantifies the uncertainty of population characteristics. It is observed that\n",
    "the model is efficient by training it on different patterns in a dataset to\n",
    "guarantee that variances are taken care of. It's also done when models need\n",
    "to be verified using random subsets or tests with labels substituted on data\n",
    "points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resampling** is a technique used in statistics and machine learning to repeatedly draw samples from a dataset to assess the variability of a statistic or to improve the generalization performance of a model. Here are a couple of scenarios where resampling is commonly employed, along with examples:\n",
    "\n",
    "1. **Assessing Model Performance:**\n",
    "   - **Scenario:** When evaluating the performance of a predictive model on a limited dataset, resampling techniques such as cross-validation or bootstrapping can be used.\n",
    "   - **Example:**\n",
    "     - Consider a dataset with 1000 observations. Instead of training and evaluating a model on the entire dataset, you can use k-fold cross-validation. If you choose 5-fold cross-validation, the dataset is divided into 5 subsets. The model is trained and tested five times, each time using a different subset for testing and the remaining data for training. This helps provide a more robust estimate of the model's performance.\n",
    "\n",
    "2. **Dealing with Limited Data:**\n",
    "   - **Scenario:** When the available data is limited, resampling techniques like bootstrapping can be used to generate additional datasets for more stable estimates.\n",
    "   - **Example:**\n",
    "     - Suppose you have a small dataset of 100 observations. To assess the variability of a statistic (e.g., mean or median), you can create multiple bootstrap samples by randomly drawing, with replacement, from the original dataset. Each bootstrap sample is of the same size as the original dataset (100 observations). By calculating the statistic of interest for each bootstrap sample, you can estimate the variability and uncertainty associated with the statistic.\n",
    "\n",
    "3. **Model Training with Imbalanced Data:**\n",
    "   - **Scenario:** In classification tasks with imbalanced classes, where one class has significantly fewer samples, resampling techniques such as oversampling the minority class or undersampling the majority class can be applied to balance the class distribution.\n",
    "   - **Example:**\n",
    "     - Consider a fraud detection problem where fraudulent transactions are rare. If the dataset has 95% non-fraudulent and 5% fraudulent transactions, you may use oversampling (replicating instances of the minority class) or undersampling (randomly removing instances from the majority class) to balance the class distribution, improving the model's ability to detect fraud.\n",
    "\n",
    "In summary, resampling is employed in various scenarios, such as model evaluation, dealing with limited data, and addressing imbalanced datasets, to enhance the robustness, reliability, and generalization capabilities of statistical analyses and machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. What does it mean to have \"imbalanced data\"?\n",
    "A data is highly imbalanced when the data is unevenly distributed across\n",
    "several categories. These datasets cause a performance problem in the\n",
    "model and inaccuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Do the predicted value, and the mean value varies in any way?\n",
    "Although there aren't many variations between these two, it's worth noting\n",
    "that they're employed in different situations. In general, the mean value\n",
    "talks about the probability distribution; in contrast, the anticipated value is\n",
    "used when dealing with random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. What does Survivorship bias mean to you?\n",
    "Due to a lack of prominence, this bias refers to the logical fallacy of\n",
    "focusing on parts that survived a procedure while missing others that did\n",
    "not. This bias can lead to incorrect conclusions being drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Survivorship bias** refers to the error that occurs when the analysis of a dataset is based only on the surviving or successful subjects, excluding those that did not \"survive\" or were unsuccessful. It is a form of selection bias that can lead to distorted conclusions and inaccurate insights because it overlooks the complete set of data.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a study on the success factors of businesses, focusing only on those that have survived and thrived. If the analysis is based solely on successful businesses, it might identify certain characteristics or strategies common among them. However, this analysis neglects the businesses that failed, and the identified factors may not be applicable or representative of the broader population of businesses.\n",
    "\n",
    "**Key Points about Survivorship Bias:**\n",
    "\n",
    "1. **Incomplete Picture:** Survivorship bias results in an incomplete and skewed view of the data because it only considers the outcomes of those who survived or succeeded.\n",
    "\n",
    "2. **Misleading Conclusions:** Conclusions drawn from survivorship-biased data can be misleading and may not generalize well to the entire population.\n",
    "\n",
    "3. **Common in Historical Analyses:** Survivorship bias is often encountered in historical analyses, especially when examining success stories or examples from the past.\n",
    "\n",
    "4. **Mitigation Strategies:** To mitigate survivorship bias, it's important to consider both successful and unsuccessful cases, understand the reasons for failure, and analyze the entire dataset to avoid drawing conclusions based solely on survivors.\n",
    "\n",
    "5. **Applications in Finance:** In finance, survivorship bias is relevant when analyzing historical stock performance. Ignoring stocks that have delisted or gone bankrupt can lead to an overestimation of historical returns.\n",
    "\n",
    "Being aware of survivorship bias is crucial in various fields, including business, finance, and research, to ensure that analyses and conclusions are based on a more comprehensive understanding of the complete dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Define key performance indicators (KPIs), lift, model fitting, robustness, and design of experiment (DOE).\n",
    "KPI is a metric that assesses how successfully a company meets its goals.\n",
    "\n",
    "Lift measures the target model's performance compared to a random choice\n",
    "model. The lift represents how well the model predicts compared to if there\n",
    "was no model.\n",
    "\n",
    "Model fitting measures how well the model under consideration matches\n",
    "the data.\n",
    "\n",
    "Robustness refers to the system's capacity to successfully handle variations\n",
    "and variances.\n",
    "\n",
    "DOE refers to the work of describing and explaining information variance\n",
    "under postulated settings to reflect factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Identify confounding variables\n",
    "Another name for confounding variables is confounders. They are\n",
    "extraneous variables that impact both independent and dependent variables,\n",
    "generating erroneous associations and mathematical correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. What distinguishes time-series issues from other regression problems?\n",
    "Time series data could be considered an extension of linear regression,\n",
    "which uses terminology such as autocorrelation and average movement to\n",
    "summarize previous data of y-axis variables to forecast a better future.\n",
    "\n",
    "Time series issues' major purpose is to forecast and predict when exact\n",
    "forecasts could be produced, but the determinant factors are not always\n",
    "known.\n",
    "\n",
    "The presence of Time in an issue might not determine that it is a time series\n",
    "problem. To be determined that an issue is a time series problem, there must\n",
    "be a relationship between target and Time.\n",
    "\n",
    "The observations that are closer in time are anticipated to be comparable to\n",
    "those far away, providing seasonality accountability. Today's weather, for\n",
    "example, would be in comparison to tomorrow's weather but not to weather\n",
    "four months from now. As a result, forecasting the weather based on\n",
    "historical data becomes a time series challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinguishing Time Series Issues from Other Regression Problems:**\n",
    "\n",
    "1. **Temporal Dependence:**\n",
    "   - **Time Series:** Observations are chronologically ordered, and each data point is dependent on previous observations. There is a temporal structure that needs to be considered.\n",
    "   - **Other Regression Problems:** Observations are assumed to be independent, and the order of data points does not matter.\n",
    "\n",
    "2. **Autocorrelation:**\n",
    "   - **Time Series:** Autocorrelation, where a variable is correlated with its past values, is a common issue. The residuals from one time point may be correlated with residuals from nearby time points.\n",
    "   - **Other Regression Problems:** Autocorrelation is not typically a concern because independence of observations is assumed.\n",
    "\n",
    "3. **Seasonality:**\n",
    "   - **Time Series:** Seasonal patterns, recurring trends, or cycles may exist in the data. Identifying and addressing seasonality is crucial.\n",
    "   - **Other Regression Problems:** Seasonality is not a common consideration in standard regression problems.\n",
    "\n",
    "4. **Trend:**\n",
    "   - **Time Series:** Long-term trends in the data can impact the overall pattern. Detecting and modeling trends is often essential.\n",
    "   - **Other Regression Problems:** Trends are not explicitly considered in standard regression, where each observation is assumed to be independent.\n",
    "\n",
    "5. **Time-Dependent Features:**\n",
    "   - **Time Series:** Features may exhibit time-dependent behavior, and their importance or influence can change over time.\n",
    "   - **Other Regression Problems:** Features are assumed to have a constant impact throughout the dataset.\n",
    "\n",
    "6. **Stationarity:**\n",
    "   - **Time Series:** Stationarity (constant mean and variance over time) is often desired. Non-stationarity can affect model performance.\n",
    "   - **Other Regression Problems:** Stationarity is not a typical concern in standard regression.\n",
    "\n",
    "7. **Forecasting:**\n",
    "   - **Time Series:** The primary goal is often forecasting future values based on historical data.\n",
    "   - **Other Regression Problems:** The focus is on understanding relationships between variables rather than predicting future values.\n",
    "\n",
    "8. **Data Splits:**\n",
    "   - **Time Series:** Data splitting for training and testing requires careful consideration of temporal order to mimic real-world scenarios.\n",
    "   - **Other Regression Problems:** Random data splits are commonly used for training and testing.\n",
    "\n",
    "Understanding these distinctions is crucial when working with time series data to ensure that the modeling approach accounts for temporal dependencies, seasonality, and other time-related patterns that may not be present in other types of regression problems. Time series analysis often involves specialized techniques such as autoregressive integrated moving average (ARIMA) models, seasonal decomposition of time series (STL), or machine learning models designed for sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. What if a dataset contains variables with more than 30% missing\n",
    "values? How would you deal with such a dataset?\n",
    "We use one of the following methods, depending on the size of the dataset:\n",
    "If the datasets are minimal, the missing values are replaced with the average\n",
    "or mean of the remaining data. This may be done in pandas by using mean\n",
    "= df. Mean (), where df is the panda's data frame that contains the dataset\n",
    "and mean () determines the data's mean. We may use df.Fillna to fill in the\n",
    "missing numbers with the computed mean (mean).\n",
    "The rows with missing values may be deleted from bigger datasets, and the\n",
    "remaining data can be utilized for data prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a dataset contains variables with more than 30% missing values, it's important to address the missing data to ensure the reliability and validity of subsequent analyses. Here are some common strategies for dealing with such datasets:\n",
    "\n",
    "1. **Understanding the Reasons for Missing Data:**\n",
    "   - Investigate the reasons for missing data. Are they missing completely at random, at random, or not at random? Understanding the nature of missingness can guide appropriate handling strategies.\n",
    "\n",
    "2. **Data Imputation:**\n",
    "   - **Mean or Median Imputation:** Replace missing values with the mean or median of the observed values for that variable.\n",
    "   - **Model-Based Imputation:** Use statistical models (e.g., regression models) to predict missing values based on other variables.\n",
    "   - **K-Nearest Neighbors (KNN) Imputation:** Impute missing values by borrowing information from the k-nearest neighbors in the feature space.\n",
    "\n",
    "3. **Deletion Strategies:**\n",
    "   - **Listwise Deletion (Complete Case Analysis):** Remove entire rows with missing values. This can lead to a substantial loss of data.\n",
    "   - **Pairwise Deletion:** Analyze available data for each pair of variables, disregarding missing values in other variables.\n",
    "\n",
    "4. **Creating Missing Data Indicators:**\n",
    "   - Introduce a binary indicator variable to denote whether a value is missing. This allows the model to account for missingness explicitly.\n",
    "\n",
    "5. **Segmentation and Imputation:**\n",
    "   - Divide the dataset into segments based on patterns of missingness. Impute missing values separately within each segment.\n",
    "\n",
    "6. **Consider the Impact on Analysis:**\n",
    "   - Assess the potential impact of missing data on the research question or analysis. In some cases, if the missingness is not random, it may introduce bias.\n",
    "\n",
    "7. **Advanced Imputation Techniques:**\n",
    "   - Use more sophisticated imputation methods such as Multiple Imputation, which generates multiple datasets with imputed values to account for uncertainty.\n",
    "\n",
    "8. **Domain-Specific Imputation:**\n",
    "   - If applicable, domain-specific knowledge or expert input may guide imputation methods, especially when certain patterns of missingness are expected.\n",
    "\n",
    "9. **Machine Learning-Based Imputation:**\n",
    "   - Train a machine learning model to predict missing values based on other features in the dataset.\n",
    "\n",
    "10. **Consulting Guidelines:**\n",
    "    - Follow guidelines and best practices in your field of study or industry when dealing with missing data.\n",
    "\n",
    "It's essential to carefully choose the imputation strategy based on the characteristics of the data and the goals of the analysis. Additionally, transparently report the handling of missing data to ensure the reproducibility and validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
